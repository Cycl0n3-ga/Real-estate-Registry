#!/usr/bin/env python3
"""
å°ç£ä¸å‹•ç”¢å¯¦åƒ¹ç™»éŒ„è³‡æ–™è½‰æ›è…³æœ¬ v3

æ•´åˆ CSV (ALL_lvr_land_a.csv) èˆ‡ API DB (transactions_all_original.db)
å…©ç¨®è³‡æ–™ä¾†æºï¼Œè¼¸å‡ºçµ±ä¸€æ ¼å¼çš„ land_data.dbã€‚

æ”¯æ´ä¸‰ç¨®è³‡æ–™ä¾†æºæ¨¡å¼:
  csv  : åƒ…å¾æ”¿åºœ CSV åŒ¯å…¥
  api  : åƒ…å¾ LVR API DB åŒ¯å…¥
  both : CSV + API åˆä½µâ€”â€”å…ˆåŒ¯å…¥ CSVï¼Œå†å°‡ API è³‡æ–™é…å°/è£œå……/æ–°å¢

åˆä½µç­–ç•¥ (--source both):
  Phase A â€” å»é‡æ’å…¥: æƒæ API DBï¼Œç”¨ (æ—¥æœŸ+åœ°å€) æˆ– (æ—¥æœŸ+ç¸½åƒ¹) åˆ¤æ–·
            æ˜¯å¦å·²å­˜åœ¨æ–¼ CSV è³‡æ–™ä¸­ã€‚ä¸å­˜åœ¨å‰‡æ–°å¢ï¼Œè³‡æ–™ç¼ºæå‰‡ä¸Ÿæ£„ã€‚
  Phase B â€” Enrich:   ç”¨ API çš„ lat/lngã€ç¤¾å€åã€æˆ¿å‹ç­‰æ¬„ä½è£œå……æ—¢æœ‰
            CSV è¨˜éŒ„çš„éºæ¼å€¼ã€‚ä¸‰å±¤åŒ¹é…: å…¨å€â†’æ—¥æœŸ+ç¸½åƒ¹â†’å»æ¨“å±¤åŸºç¤åœ°å€ã€‚
  Phase C â€” ç¤¾å€å›å¡«: å¾ API ç¤¾å€å°æ‡‰è¡¨å›å¡« community_nameã€‚

ç”¨æ³•:
  python3 convert.py                          # é è¨­: both
  python3 convert.py --source csv             # åƒ… CSV
  python3 convert.py --source api             # åƒ… API
  python3 convert.py --source both            # CSV + API åˆä½µ
  python3 convert.py --csv-input a.csv --api-input t.db --output out.db
"""

import csv
import json
import sqlite3
import os
import sys
import argparse
import re
import time

# â”€â”€ å…±ç”¨æ¨¡çµ„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
sys.path.insert(0, os.path.join(os.path.dirname(os.path.abspath(__file__)), '..'))
from address_utils import (
    normalize_address,
    parse_address,
    chinese_numeral_to_int,
    fullwidth_to_halfwidth,
    CITY_CODE_MAP,
    AMBIGUOUS_DISTRICTS,
    DISTRICT_CITY_MAP,
)

# å‘å¾Œç›¸å®¹åˆ¥å (ä¾› test_convert.py ç­‰ä½¿ç”¨)
normalize_address_numbers = normalize_address


# ============================================================
# å®‰å…¨æ•¸å€¼è½‰æ›
# ============================================================

def safe_int(val, default=None):
    if val is None or val == '':
        return default
    try:
        return int(float(str(val).replace(',', '').replace(' ', '')))
    except (ValueError, TypeError):
        return default


def safe_float(val, default=None):
    if val is None or val == '':
        return default
    try:
        return float(str(val).replace(',', ''))
    except (ValueError, TypeError):
        return default


def parse_price(val):
    """'39,380,000' â†’ int"""
    if not val:
        return None
    try:
        return int(str(val).replace(',', '').replace(' ', ''))
    except Exception:
        return None


# ============================================================
# API è³‡æ–™è§£æå·¥å…·
# ============================================================

# ä¸­æ–‡æ¨“å±¤ â†’ æ•¸å­—ï¼ˆç”¨æ–¼è§£æ raw_json çš„ 'f' æ¬„ä½ï¼‰
CHINESE_FLOOR = {
    'ä¸€': 1, 'äºŒ': 2, 'ä¸‰': 3, 'å››': 4, 'äº”': 5, 'å…­': 6, 'ä¸ƒ': 7,
    'å…«': 8, 'ä¹': 9, 'å': 10, 'åä¸€': 11, 'åäºŒ': 12, 'åä¸‰': 13,
    'åå››': 14, 'åäº”': 15, 'åå…­': 16, 'åä¸ƒ': 17, 'åå…«': 18,
    'åä¹': 19, 'äºŒå': 20, 'äºŒåä¸€': 21, 'äºŒåäºŒ': 22, 'äºŒåä¸‰': 23,
    'äºŒåå››': 24, 'äºŒåäº”': 25, 'äºŒåå…­': 26, 'äºŒåä¸ƒ': 27,
    'äºŒåå…«': 28, 'äºŒåä¹': 29, 'ä¸‰å': 30,
    'åœ°ä¸‹ä¸€': -1, 'åœ°ä¸‹äºŒ': -2, 'åœ°ä¸‹ä¸‰': -3,
}


def parse_floor_info(floor_str):
    """è§£ææ¨“å±¤æ¬„ä½: 'ä¹å±¤/åäº”å±¤' â†’ ('9', '15')"""
    if not floor_str:
        return '', ''
    parts = floor_str.split('/')
    if len(parts) == 2:
        fl, tf = parts[0].strip(), parts[1].strip()
        for s, attr in ((fl, 'fl'), (tf, 'tf')):
            stripped = s.rstrip('å±¤')
            if stripped in CHINESE_FLOOR:
                if attr == 'fl':
                    fl = str(CHINESE_FLOOR[stripped])
                else:
                    tf = str(CHINESE_FLOOR[stripped])
        return fl, tf
    return floor_str.strip(), ''


def normalize_date(date_str):
    """'101/01/09' â†’ '1010109'"""
    if not date_str:
        return ''
    return date_str.replace('/', '')


def clean_trans_addr(addr_raw):
    """å– transactions.db åœ°å€ '#' å¾ŒåŠéƒ¨çš„ä¹¾æ·¨åœ°å€"""
    if addr_raw and '#' in addr_raw:
        return addr_raw.split('#', 1)[1]
    return addr_raw or ''


def norm_addr_simple(addr):
    """ç°¡å–®æ­£è¦åŒ–: å…¨å½¢â†’åŠå½¢ã€è‡ºâ†’å°ã€å»ç©ºç™½"""
    return fullwidth_to_halfwidth(addr or '').replace('è‡º', 'å°').replace(' ', '')


def strip_city(addr):
    """ç§»é™¤åœ°å€é–‹é ­çš„ç¸£å¸‚å"""
    for city in CITY_CODE_MAP.values():
        if addr.startswith(city):
            return addr[len(city):]
    for old in ('å°åŒ—ç¸£', 'æ¡ƒåœ’ç¸£', 'å°ä¸­ç¸£', 'å°å—ç¸£', 'é«˜é›„ç¸£'):
        if addr.startswith(old):
            return addr[len(old):]
    return addr


def strip_floor(addr):
    """å»é™¤å°¾ç«¯æ¨“å±¤è³‡è¨Šï¼Œå–å¾—å»ºç‰©åŸºç¤åœ°å€"""
    addr = re.sub(r'(-\d+|åœ°ä¸‹\d+|\d+)æ¨“[ä¹‹\d]*$', '', addr)
    return addr.rstrip('ä¹‹å·è™Ÿ ')


# ============================================================
# è¡¨çµæ§‹ / ç´¢å¼• / FTS
# ============================================================

def create_tables(cursor):
    """å»ºç«‹ SQLite è³‡æ–™è¡¨"""
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS land_transaction (
            id              INTEGER PRIMARY KEY AUTOINCREMENT,
            raw_district    TEXT,
            transaction_type TEXT,
            address         TEXT,
            land_area       REAL,
            urban_zone      TEXT,
            non_urban_zone  TEXT,
            non_urban_use   TEXT,
            transaction_date TEXT,
            transaction_count TEXT,
            floor_level     TEXT,
            total_floors    TEXT,
            building_type   TEXT,
            main_use        TEXT,
            main_material   TEXT,
            build_date      TEXT,
            building_area   REAL,
            rooms           INTEGER,
            halls           INTEGER,
            bathrooms       INTEGER,
            partitioned     TEXT,
            has_management  TEXT,
            total_price     INTEGER,
            unit_price      REAL,
            parking_type    TEXT,
            parking_area    REAL,
            parking_price   INTEGER,
            note            TEXT,
            serial_no       TEXT,
            main_area       REAL,
            attached_area   REAL,
            balcony_area    REAL,
            elevator        TEXT,
            transfer_no     TEXT,
            county_city     TEXT,
            district        TEXT,
            village         TEXT,
            street          TEXT,
            lane            TEXT,
            alley           TEXT,
            number          TEXT,
            floor           TEXT,
            sub_number      TEXT,
            community_name  TEXT,
            lat             REAL,
            lng             REAL
        )
    ''')


def create_indexes(cursor):
    """å»ºç«‹æœå°‹ç´¢å¼•"""
    print('  ğŸ“‡ å»ºç«‹ç´¢å¼•...')
    indexes = [
        ('idx_county_city', 'county_city'),
        ('idx_district', 'district'),
        ('idx_street', 'street'),
        ('idx_lane', 'lane'),
        ('idx_number', 'number'),
        ('idx_floor', 'floor'),
        ('idx_date', 'transaction_date'),
        ('idx_price', 'total_price'),
        ('idx_serial', 'serial_no'),
    ]
    for name, col in indexes:
        cursor.execute(f'CREATE INDEX IF NOT EXISTS {name} ON land_transaction({col})')
    cursor.execute('''CREATE INDEX IF NOT EXISTS idx_addr_combo
        ON land_transaction(county_city, district, street, lane, number)''')


def create_fts(cursor):
    """å»ºç«‹ FTS5 å…¨æ–‡æª¢ç´¢è¡¨"""
    print('  ğŸ” å»ºç«‹ FTS5 å…¨æ–‡æª¢ç´¢...')
    cursor.execute('DROP TABLE IF EXISTS address_fts')
    cursor.execute('''
        CREATE VIRTUAL TABLE address_fts USING fts5(
            address,
            content='land_transaction',
            content_rowid='id',
            tokenize='unicode61'
        )
    ''')
    cursor.execute('''
        INSERT INTO address_fts(rowid, address)
        SELECT id, address FROM land_transaction WHERE address != ''
    ''')


# 45 æ¬„ä½ INSERT èªå¥
INSERT_SQL = '''INSERT INTO land_transaction (
    raw_district, transaction_type, address, land_area,
    urban_zone, non_urban_zone, non_urban_use,
    transaction_date, transaction_count, floor_level, total_floors,
    building_type, main_use, main_material, build_date,
    building_area, rooms, halls, bathrooms, partitioned,
    has_management, total_price, unit_price,
    parking_type, parking_area, parking_price,
    note, serial_no, main_area, attached_area, balcony_area,
    elevator, transfer_no,
    county_city, district, village, street, lane, alley,
    number, floor, sub_number,
    community_name, lat, lng
) VALUES (?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?)'''


# ============================================================
# Phase 1: å¾ CSV è¼‰å…¥
# ============================================================

def load_csv(conn, csv_path):
    """å¾ ALL_lvr_land_a.csv è¼‰å…¥è³‡æ–™åˆ° land_data.db"""
    print(f'\nğŸ“„ [CSV] è¼‰å…¥: {csv_path}')
    cursor = conn.cursor()

    batch = []
    batch_size = 10000
    total = 0
    parsed_ok = 0
    t0 = time.time()

    with open(csv_path, 'r', encoding='utf-8-sig') as f:
        reader = csv.reader(f)
        next(reader, None)  # ä¸­æ–‡æ¨™é ­
        next(reader, None)  # è‹±æ–‡æ¨™é ­

        for row in reader:
            total += 1
            while len(row) < 33:
                row.append('')

            raw_address = row[2]
            parsed = parse_address(raw_address, row[0])
            if parsed['street']:
                parsed_ok += 1

            values = (
                row[0],                          # raw_district
                row[1],                          # transaction_type
                row[2],                          # address
                safe_float(row[3]),              # land_area
                row[4],                          # urban_zone
                row[5],                          # non_urban_zone
                row[6],                          # non_urban_use
                row[7],                          # transaction_date
                row[8],                          # transaction_count
                row[9],                          # floor_level
                row[10],                         # total_floors
                row[11],                         # building_type
                row[12],                         # main_use
                row[13],                         # main_material
                row[14],                         # build_date
                safe_float(row[15]),             # building_area
                safe_int(row[16]),               # rooms
                safe_int(row[17]),               # halls
                safe_int(row[18]),               # bathrooms
                row[19],                         # partitioned
                row[20],                         # has_management
                safe_int(row[21]),               # total_price
                safe_float(row[22]),             # unit_price
                row[23],                         # parking_type
                safe_float(row[24]),             # parking_area
                safe_int(row[25]),               # parking_price
                row[26],                         # note
                row[27],                         # serial_no
                safe_float(row[28]),             # main_area
                safe_float(row[29]),             # attached_area
                safe_float(row[30]),             # balcony_area
                row[31],                         # elevator
                row[32],                         # transfer_no
                parsed['county_city'],
                parsed['district'],
                parsed['village'],
                parsed['street'],
                parsed['lane'],
                parsed['alley'],
                parsed['number'],
                parsed['floor'],
                parsed['sub_number'],
                None, None, None,                # community_name, lat, lng
            )
            batch.append(values)

            if len(batch) >= batch_size:
                cursor.executemany(INSERT_SQL, batch)
                conn.commit()
                elapsed = time.time() - t0
                rate = total / elapsed if elapsed > 0 else 0
                print(f'\r  â³ å·²è™•ç† {total:,} ç­† ({rate:,.0f} ç­†/ç§’)',
                      end='', flush=True)
                batch = []

    if batch:
        cursor.executemany(INSERT_SQL, batch)
        conn.commit()

    elapsed = time.time() - t0
    pct = parsed_ok / total * 100 if total else 0
    print(f'\n  âœ… CSV è¼‰å…¥å®Œæˆ: {total:,} ç­†, '
          f'åœ°å€è§£æç‡ {pct:.1f}%, {elapsed:.1f}s')
    return total


# ============================================================
# Phase 2: å¾ API DB è¼‰å…¥ (api-only æ¨¡å¼)
# ============================================================

def _parse_api_record(row):
    """
    å°‡ transactions.db ä¸€åˆ— â†’ INSERT ç”¨ 45-tupleã€‚
    ä½¿ç”¨ parse_address + city_hint åšå®Œæ•´çµæ§‹åŒ–è§£æã€‚
    å›å‚³ None è¡¨ç¤ºè³‡æ–™ç¼ºææ‡‰ä¸Ÿæ£„ã€‚
    """
    _id, city_code, town, addr_raw, build_type, community, \
        date_str, floor_col, area_col, tp_raw, up_raw, \
        lat, lon, sq, rj_text = row

    j = {}
    if rj_text:
        try:
            j = json.loads(rj_text)
        except Exception:
            pass

    # åœ°å€æ¸…æ´—
    addr_clean = clean_trans_addr(addr_raw)
    if not addr_clean or 'è™Ÿ' not in addr_clean:
        return None

    # ç”¨ city_code å–å¾— city_hint â†’ ç²¾ç¢ºæ¶ˆæ­§
    city_hint = CITY_CODE_MAP.get(city_code, '')
    parsed = parse_address(addr_clean, city_hint=city_hint)

    # æ—¥æœŸ
    transaction_date = normalize_date(date_str)

    # æ¨“å±¤
    floor_json = j.get('f', '') or floor_col or ''
    floor_level, total_floors = parse_floor_info(floor_json)

    # JSON æ¬„ä½
    transaction_type = j.get('t', '') or ''
    rooms = safe_int(j.get('j'))
    halls = safe_int(j.get('k'))
    bathrooms = safe_int(j.get('l'))
    has_management = j.get('m', '') or ''
    main_use = j.get('pu', '') or j.get('AA11', '') or ''
    building_type_j = build_type or j.get('b', '') or ''
    note = j.get('note', '') or ''

    total_price = parse_price(tp_raw) or parse_price(j.get('tp'))
    unit_price = safe_float(up_raw) or safe_float(j.get('cp'))
    building_area = safe_float(area_col) or safe_float(j.get('s'))

    serial_no = f'api_{sq}' if sq else None

    lat_val = lat if (lat and lat != 0) else None
    lng_val = lon if (lon and lon != 0) else None
    if not lat_val and j.get('lat'):
        lat_val = j['lat']
    if not lng_val and j.get('lon'):
        lng_val = j['lon']

    floor_parsed = parsed['floor']
    if not floor_parsed and floor_level:
        try:
            int(floor_level)
            floor_parsed = floor_level
        except ValueError:
            pass

    return (
        parsed.get('district') or town or '',  # raw_district
        transaction_type,                       # transaction_type
        addr_clean,                             # address
        None,                                   # land_area
        '', '', '',                             # urban / non-urban zones
        transaction_date,                       # transaction_date
        '',                                     # transaction_count
        floor_level,                            # floor_level
        total_floors,                           # total_floors
        building_type_j,                        # building_type
        main_use,                               # main_use
        '',                                     # main_material
        '',                                     # build_date
        building_area,                          # building_area
        rooms,                                  # rooms
        halls,                                  # halls
        bathrooms,                              # bathrooms
        '',                                     # partitioned
        has_management,                         # has_management
        total_price,                            # total_price
        unit_price,                             # unit_price
        '',                                     # parking_type
        None,                                   # parking_area
        None,                                   # parking_price
        note,                                   # note
        serial_no,                              # serial_no
        None,                                   # main_area
        None,                                   # attached_area
        None,                                   # balcony_area
        '',                                     # elevator
        '',                                     # transfer_no
        parsed['county_city'],                  # county_city
        parsed['district'],                     # district
        parsed['village'],                      # village
        parsed['street'],                       # street
        parsed['lane'],                         # lane
        parsed['alley'],                        # alley
        parsed['number'],                       # number
        floor_parsed,                           # floor
        parsed['sub_number'],                   # sub_number
        community or '',                        # community_name
        lat_val,                                # lat
        lng_val,                                # lng
    )


def load_api(conn, api_db_path):
    """å¾ transactions_all_original.db è¼‰å…¥è³‡æ–™åˆ° land_data.db (api-only)"""
    print(f'\nğŸŒ [API] è¼‰å…¥: {api_db_path}')
    cursor = conn.cursor()

    conn_t = sqlite3.connect(api_db_path)
    conn_t.text_factory = lambda b: b.decode('utf-8', errors='replace')
    ct = conn_t.cursor()
    ct.execute(
        'SELECT id, city, town, address, build_type, community, date_str, '
        'floor, area, total_price, unit_price, lat, lon, sq, raw_json '
        'FROM transactions'
    )

    batch = []
    batch_size = 10000
    total = inserted = skipped = 0
    t0 = time.time()

    for row in ct:
        total += 1
        try:
            rec = _parse_api_record(row)
        except Exception:
            skipped += 1
            continue
        if rec is None:
            skipped += 1
            continue

        batch.append(rec)
        inserted += 1

        if len(batch) >= batch_size:
            cursor.executemany(INSERT_SQL, batch)
            conn.commit()
            elapsed = time.time() - t0
            rate = total / elapsed if elapsed > 0 else 0
            print(f'\r  â³ æƒæ {total:,} | æ’å…¥ {inserted:,} | '
                  f'ç•¥é {skipped:,} ({rate:,.0f}/s)',
                  end='', flush=True)
            batch = []

    if batch:
        cursor.executemany(INSERT_SQL, batch)
        conn.commit()

    conn_t.close()
    elapsed = time.time() - t0
    print(f'\n  âœ… API è¼‰å…¥å®Œæˆ: æƒæ {total:,}, '
          f'æ’å…¥ {inserted:,}, ç•¥é {skipped:,}, {elapsed:.1f}s')
    return inserted


# ============================================================
# Phase 3: åˆä½µæ¨¡å¼ â€” CSV ç‚ºä¸», API é…å°/è£œå……/æ–°å¢
# ============================================================

# Enrich ç”¨æ¬„ä½å®šç¾©: (land_data æ¬„ä½, edata key, åˆ¤ç©ºå‡½æ•¸)
ENRICH_FIELDS = [
    ('lat',              'lat',              lambda v: v is None or v == 0),
    ('lng',              'lng',              lambda v: v is None or v == 0),
    ('community_name',   'community',        lambda v: not v),
    ('county_city',      'county_city',      lambda v: not v),
    ('building_type',    'building_type',    lambda v: not v),
    ('main_use',         'main_use',         lambda v: not v),
    ('has_management',   'has_management',   lambda v: not v),
    ('rooms',            'rooms',            lambda v: v is None),
    ('halls',            'halls',            lambda v: v is None),
    ('bathrooms',        'bathrooms',        lambda v: v is None),
    ('building_area',    'building_area',    lambda v: v is None or v == 0),
    ('unit_price',       'unit_price',       lambda v: v is None or v == 0),
    ('transaction_type', 'transaction_type', lambda v: not v),
    ('floor_level',      'floor_level',      lambda v: not v),
    ('total_floors',     'total_floors',     lambda v: not v),
    ('note',             'note',             lambda v: not v),
]


def _build_csv_keys(cursor):
    """å¾å·²è¼‰å…¥çš„ CSV è³‡æ–™å»ºç«‹å»é‡ç”¨çš„ key set"""
    print('  å»ºç«‹å»é‡éµå€¼...', flush=True)
    cursor.execute(
        'SELECT transaction_date, address, total_price '
        'FROM land_transaction WHERE address LIKE "%è™Ÿ%"'
    )
    addr_keys = set()
    price_keys = set()
    for date, addr, price in cursor.fetchall():
        d = (date or '').replace('/', '')[:7]
        a = strip_city(norm_addr_simple(addr or ''))
        addr_keys.add((d, a))
        p = parse_price(price)
        if p:
            price_keys.add((d, p))
    print(f'    date+addr keys: {len(addr_keys):,}, '
          f'date+price keys: {len(price_keys):,}')
    return addr_keys, price_keys


def _richness(d):
    """è¨ˆç®— edata çš„ã€Œè³‡æ–™è±å¯Œåº¦ã€åˆ†æ•¸"""
    s = 0
    if d.get('lat') and d['lat'] != 0:
        s += 3
    if d.get('community'):
        s += 3
    for k in ('rooms', 'halls', 'bathrooms', 'building_area',
              'building_type', 'main_use', 'has_management',
              'transaction_type'):
        if d.get(k) not in (None, '', 0):
            s += 1
    return s


def _merge_edata(target, source):
    """å¾ source è£œå…… target ä¸­çš„ç©ºæ¬„ä½"""
    for f in target:
        tv = target.get(f)
        if tv is None or tv == '' or tv == 0:
            sv = source.get(f)
            if sv is not None and sv != '' and sv != 0:
                target[f] = sv


def _build_enrich_maps(api_db_path):
    """
    å¾ API DB å»ºç«‹ä¸‰ç¨®æ˜ å°„è¡¨ä¾› enrich ä½¿ç”¨:
      full_map       : æ­£è¦åŒ–å®Œæ•´åœ°å€ â†’ edata
      date_price_map : (æ—¥æœŸ, ç¸½åƒ¹) â†’ edata
      base_map       : å»æ¨“å±¤åŸºç¤åœ°å€ â†’ edata
    """
    print('  å»ºç«‹ API æ˜ å°„è¡¨...', flush=True)
    conn = sqlite3.connect(api_db_path)
    conn.text_factory = lambda b: b.decode('utf-8', errors='replace')
    cur = conn.cursor()
    cur.execute("""
        SELECT city, address, lat, lon, community, build_type,
               date_str, floor, area, total_price, unit_price, raw_json
        FROM transactions
        WHERE address IS NOT NULL AND address != '' AND address != '#'
    """)

    full_map = {}
    date_price_map = {}
    base_map = {}
    count = 0

    for row in cur:
        city_code, addr_raw = row[0], row[1]
        lat, lon, community = row[2], row[3], (row[4] or '').strip()
        build_type, date_str, floor_col = row[5], row[6], row[7]
        area_col, tp_raw, up_raw, rj_text = row[8], row[9], row[10], row[11]

        j = {}
        if rj_text:
            try:
                j = json.loads(rj_text)
            except Exception:
                pass

        floor_json = j.get('f', '') or floor_col or ''
        floor_level, total_floors = parse_floor_info(floor_json)

        edata = {
            'lat': lat if (lat and lat != 0) else None,
            'lng': lon if (lon and lon != 0) else None,
            'community': community,
            'county_city': CITY_CODE_MAP.get(city_code, ''),
            'building_type': build_type or j.get('b', '') or '',
            'main_use': j.get('pu', '') or j.get('AA11', '') or '',
            'has_management': j.get('m', '') or '',
            'rooms': safe_int(j.get('j')),
            'halls': safe_int(j.get('k')),
            'bathrooms': safe_int(j.get('l')),
            'building_area': safe_float(area_col) or safe_float(j.get('s')),
            'unit_price': safe_float(up_raw) or safe_float(j.get('cp')),
            'transaction_type': j.get('t', '') or '',
            'floor_level': floor_level,
            'total_floors': total_floors,
            'note': j.get('note', '') or '',
        }

        clean = clean_trans_addr(addr_raw)
        norm = strip_city(norm_addr_simple(clean))
        if not norm:
            continue
        base = strip_floor(norm)
        date_norm = normalize_date(date_str)
        total_price = parse_price(tp_raw) or parse_price(j.get('tp'))

        # å…¨å€æ˜ å°„ (å– richness è¼ƒé«˜è€…)
        if norm not in full_map or _richness(edata) > _richness(full_map[norm]):
            if norm in full_map:
                _merge_edata(edata, full_map[norm])
            full_map[norm] = edata
        else:
            _merge_edata(full_map[norm], edata)

        # æ—¥æœŸ+ç¸½åƒ¹æ˜ å°„
        if date_norm and total_price and total_price > 0:
            key = (date_norm, total_price)
            if key not in date_price_map or \
               _richness(edata) > _richness(date_price_map[key]):
                if key in date_price_map:
                    _merge_edata(edata, date_price_map[key])
                date_price_map[key] = edata
            else:
                _merge_edata(date_price_map[key], edata)

        # åŸºç¤åœ°å€ (å»æ¨“å±¤)
        if base and base != norm:
            if base not in base_map or \
               _richness(edata) > _richness(base_map[base]):
                if base in base_map:
                    _merge_edata(edata, base_map[base])
                base_map[base] = edata
            else:
                _merge_edata(base_map[base], edata)

        count += 1
        if count % 500_000 == 0:
            print(f'    å·²è®€å– {count:,} ç­†...', flush=True)

    conn.close()
    print(f'    å®Œæˆ: {count:,} ç­†')
    print(f'    full_map: {len(full_map):,}, '
          f'date_price: {len(date_price_map):,}, '
          f'base: {len(base_map):,}')
    return full_map, date_price_map, base_map


def _flush_updates(conn, batch):
    """æ‰¹æ¬¡åŸ·è¡Œ UPDATE"""
    cur = conn.cursor()
    for updates, row_id in batch:
        set_clauses = ', '.join(f'{col} = ?' for col in updates)
        values = list(updates.values()) + [row_id]
        cur.execute(
            f'UPDATE land_transaction SET {set_clauses} WHERE id = ?',
            values
        )
    conn.commit()


def _backfill_community(conn, api_db_path):
    """
    å¾ API DB å›å¡« community_nameã€‚
    å»ºç«‹ (county_city, district, road+è™Ÿ) â†’ community æ˜ å°„ï¼Œ
    å†æ‰¹æ¬¡ UPDATE land_data.dbã€‚
    """
    print('  å›å¡«ç¤¾å€å...', flush=True)
    conn_t = sqlite3.connect(api_db_path)
    conn_t.text_factory = lambda b: b.decode('utf-8', errors='replace')
    rows = conn_t.execute(
        "SELECT city, address, community FROM transactions "
        "WHERE community != '' AND community IS NOT NULL AND address != ''"
    ).fetchall()
    conn_t.close()

    mapping = {}
    for city_code, addr_raw, community in rows:
        addr = norm_addr_simple(clean_trans_addr(addr_raw))
        short = strip_city(addr)
        m = re.match(r'^(.{1,4}?[å€é®é„‰å¸‚])', short)
        if not m:
            continue
        district = m.group(1)
        rest = short[len(district):] if short.startswith(district) else short
        pos = rest.find('è™Ÿ')
        if pos < 0:
            continue
        road_number = rest[:pos + 1]
        county_city = CITY_CODE_MAP.get(city_code, '')
        key = (county_city, district, road_number)
        if key not in mapping:
            mapping[key] = {}
        mapping[key][community] = mapping[key].get(community, 0) + 1

    comm_map = {k: max(v, key=v.get) for k, v in mapping.items()}
    print(f'    ç¤¾å€æ˜ å°„: {len(comm_map):,} å€‹é–€ç‰Œ', flush=True)

    updated = 0
    conn.execute('BEGIN')
    for i, ((county_city, district, road_number), community) in \
            enumerate(comm_map.items()):
        pattern = f'%{district}{road_number}%'
        cur = conn.execute(
            "UPDATE land_transaction SET community_name = ? "
            "WHERE district = ? AND address LIKE ? "
            "AND (community_name IS NULL OR community_name = '')",
            (community, district, pattern)
        )
        updated += cur.rowcount
        if (i + 1) % 500 == 0:
            conn.execute('COMMIT')
            conn.execute('BEGIN')
    conn.execute('COMMIT')
    return updated


def merge_api(conn, api_db_path):
    """
    åˆä½µ API è³‡æ–™åˆ°å·²æœ‰ CSV è³‡æ–™çš„ land_data.dbã€‚

    Phase A: æƒæ API DBï¼Œä»¥ (æ—¥æœŸ+åœ°å€) æˆ– (æ—¥æœŸ+ç¸½åƒ¹) å»é‡ï¼Œ
             ä¸å­˜åœ¨å‰‡æ–°å¢ï¼Œè³‡æ–™ç¼ºæå‰‡ä¸Ÿæ£„ã€‚
    Phase B: å»ºç«‹ API æ˜ å°„è¡¨ï¼Œenrich æ—¢æœ‰ CSV è¨˜éŒ„çš„ç¼ºå¤±æ¬„ä½ã€‚
    Phase C: å›å¡«ç¤¾å€åã€‚
    """
    print(f'\nğŸ”— [åˆä½µ] å°‡ API è³‡æ–™æ•´åˆåˆ° CSV è³‡æ–™...')
    cursor = conn.cursor()
    t0 = time.time()

    # â”€â”€ Phase A: æ’å…¥ API ç¨æœ‰è¨˜éŒ„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    addr_keys, price_keys = _build_csv_keys(cursor)

    conn_t = sqlite3.connect(api_db_path)
    conn_t.text_factory = lambda b: b.decode('utf-8', errors='replace')
    ct = conn_t.cursor()
    ct.execute(
        'SELECT id, city, town, address, build_type, community, date_str, '
        'floor, area, total_price, unit_price, lat, lon, sq, raw_json '
        'FROM transactions'
    )

    batch = []
    total = inserted = dup_addr = dup_price = discarded = 0

    for row in ct:
        total += 1
        date_str = row[6] or ''
        addr_raw = row[3] or ''
        tp_raw = row[9]

        d = normalize_date(date_str)[:7]
        addr_clean = clean_trans_addr(addr_raw)
        addr_norm = strip_city(norm_addr_simple(addr_clean))

        # å»é‡æª¢æŸ¥
        if (d, addr_norm) in addr_keys:
            dup_addr += 1
            continue
        price = parse_price(tp_raw)
        if price and (d, price) in price_keys:
            dup_price += 1
            continue

        try:
            rec = _parse_api_record(row)
        except Exception:
            discarded += 1
            continue
        if rec is None:
            discarded += 1
            continue

        batch.append(rec)
        inserted += 1

        # æ›´æ–° key set é˜²æ­¢å¾ŒçºŒé‡è¤‡
        addr_keys.add((d, addr_norm))
        if price:
            price_keys.add((d, price))

        if len(batch) >= 10000:
            cursor.executemany(INSERT_SQL, batch)
            conn.commit()
            elapsed = time.time() - t0
            print(f'\r  [A æ’å…¥] æƒæ {total:,} | æ–°å¢ {inserted:,} | '
                  f'é‡è¤‡(åœ°å€) {dup_addr:,} | é‡è¤‡(åƒ¹æ ¼) {dup_price:,} | '
                  f'ä¸Ÿæ£„ {discarded:,} ({elapsed:.0f}s)',
                  end='', flush=True)
            batch = []

    if batch:
        cursor.executemany(INSERT_SQL, batch)
        conn.commit()

    conn_t.close()
    elapsed_a = time.time() - t0
    print(f'\n  âœ… Phase A å®Œæˆ: æ–°å¢ {inserted:,}, '
          f'é‡è¤‡è·³é {dup_addr + dup_price:,}, '
          f'ä¸Ÿæ£„ {discarded:,}, {elapsed_a:.1f}s')

    # â”€â”€ Phase B: Enrich æ—¢æœ‰ CSV è¨˜éŒ„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    t1 = time.time()
    full_map, date_price_map, base_map = _build_enrich_maps(api_db_path)

    print('  æ›´æ–° CSV è¨˜éŒ„çš„ç¼ºå¤±æ¬„ä½...', flush=True)
    enrich_cols = ', '.join(
        ['id', 'address', 'transaction_date', 'total_price'] +
        [f[0] for f in ENRICH_FIELDS]
    )
    # åªè™•ç†é API ä¾†æºçš„è¨˜éŒ„
    cursor.execute(f"""
        SELECT {enrich_cols} FROM land_transaction
        WHERE (serial_no NOT LIKE 'api_%' OR serial_no IS NULL)
          AND address LIKE '%è™Ÿ%'
    """)

    updated_full = updated_dp = updated_base = 0
    not_found = already_ok = 0
    update_batch = []
    BATCH = 5000

    for row in cursor.fetchall():
        row_id = row[0]
        address = row[1]
        trans_date = row[2]
        land_price = row[3]

        # ç•¶å‰æ¬„ä½å€¼
        current = {}
        for idx, (db_col, _, _) in enumerate(ENRICH_FIELDS):
            current[db_col] = row[4 + idx]

        needs = any(
            is_empty(current[db_col])
            for db_col, _, is_empty in ENRICH_FIELDS
        )
        if not needs:
            already_ok += 1
            continue

        # ä¸‰å±¤åŒ¹é…
        norm = strip_city(norm_addr_simple(address or ''))
        base = strip_floor(norm)
        d = normalize_date(trans_date)

        match = None
        match_type = None

        if norm in full_map:
            match = full_map[norm].copy()
            match_type = 'full'

        if d and land_price and land_price > 0:
            dp = date_price_map.get((d, land_price))
            if dp:
                if match is None:
                    match = dp.copy()
                    match_type = 'date_price'
                else:
                    _merge_edata(match, dp)

        if base and base != norm and base in base_map:
            bm = base_map[base]
            if match is None:
                match = bm.copy()
                match_type = 'base'
            else:
                _merge_edata(match, bm)

        if match is None:
            not_found += 1
            continue

        # åƒ…æ›´æ–°ç©ºæ¬„ä½
        updates = {}
        for db_col, edata_key, is_empty in ENRICH_FIELDS:
            if is_empty(current[db_col]):
                new_val = match.get(edata_key)
                if new_val is not None and new_val != '' and new_val != 0:
                    updates[db_col] = new_val

        if not updates:
            not_found += 1
            continue

        update_batch.append((updates, row_id))
        if match_type == 'full':
            updated_full += 1
        elif match_type == 'date_price':
            updated_dp += 1
        else:
            updated_base += 1

        if len(update_batch) >= BATCH:
            _flush_updates(conn, update_batch)
            update_batch = []

    if update_batch:
        _flush_updates(conn, update_batch)

    total_updated = updated_full + updated_dp + updated_base
    elapsed_b = time.time() - t1
    print(f'  âœ… Phase B å®Œæˆ: enrich {total_updated:,} ç­† '
          f'(å…¨å€:{updated_full:,} æ—¥æœŸ+åƒ¹æ ¼:{updated_dp:,} '
          f'åŸºç¤:{updated_base:,}), {elapsed_b:.1f}s')

    # â”€â”€ Phase C: å›å¡«ç¤¾å€å â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    t2 = time.time()
    bf_count = _backfill_community(conn, api_db_path)
    elapsed_c = time.time() - t2
    print(f'  âœ… Phase C å®Œæˆ: ç¤¾å€åå›å¡« {bf_count:,} ç­†, {elapsed_c:.1f}s')

    total_elapsed = time.time() - t0
    print(f'\n  ğŸ”— åˆä½µç¸½è¨ˆ: æ–°å¢ {inserted:,}, '
          f'enriched {total_updated:,}, '
          f'ç¤¾å€å›å¡« {bf_count:,}, ç¸½è€—æ™‚ {total_elapsed:.1f}s')


# ============================================================
# ä¸»è¦æµç¨‹
# ============================================================

def convert(source, csv_path=None, api_path=None, output_path=None):
    """
    ä¸»è¦è½‰æ›æµç¨‹ã€‚

    Args:
        source: 'csv', 'api', æˆ– 'both'
        csv_path:    CSV è¼¸å…¥è·¯å¾‘
        api_path:    API DB è¼¸å…¥è·¯å¾‘
        output_path: SQLite è¼¸å‡ºè·¯å¾‘
    """
    print(f'\n{"=" * 60}')
    print(f'  è³‡æ–™ä¾†æºæ¨¡å¼: {source}')
    if csv_path and source in ('csv', 'both'):
        print(f'  CSV:          {csv_path}')
    if api_path and source in ('api', 'both'):
        print(f'  API DB:       {api_path}')
    print(f'  è¼¸å‡º:         {output_path}')
    print(f'{"=" * 60}\n')

    if os.path.exists(output_path):
        os.remove(output_path)
        print('  (å·²åˆªé™¤èˆŠè³‡æ–™åº«)')

    conn = sqlite3.connect(output_path)
    cursor = conn.cursor()

    # æ•ˆèƒ½è¨­å®š
    cursor.execute('PRAGMA page_size=4096')
    cursor.execute('PRAGMA journal_mode=WAL')
    cursor.execute('PRAGMA synchronous=NORMAL')
    cursor.execute('PRAGMA cache_size=-200000')
    cursor.execute('PRAGMA temp_store=MEMORY')

    create_tables(cursor)
    conn.commit()

    t0 = time.time()

    if source == 'csv':
        load_csv(conn, csv_path)
    elif source == 'api':
        load_api(conn, api_path)
    elif source == 'both':
        load_csv(conn, csv_path)
        merge_api(conn, api_path)

    # å»ºç«‹ç´¢å¼•å’Œ FTS
    cursor = conn.cursor()
    create_indexes(cursor)
    conn.commit()
    create_fts(cursor)
    conn.commit()
    cursor.close()

    # ANALYZE
    print('\n  ğŸ“Š æ›´æ–°çµ±è¨ˆè³‡è¨Š...')
    conn.execute('ANALYZE')
    conn.commit()

    # VACUUM å£“ç¸®
    print('  ğŸ—œ  å£“ç¸®è³‡æ–™åº« (VACUUM)...')
    conn.execute('PRAGMA journal_mode=DELETE')
    conn.commit()
    conn.execute('VACUUM')
    conn.execute('PRAGMA journal_mode=WAL')
    conn.commit()

    # æœ€çµ‚çµ±è¨ˆ
    cur = conn.cursor()
    cur.execute('SELECT COUNT(*) FROM land_transaction')
    total = cur.fetchone()[0]
    cur.execute(
        'SELECT COUNT(*) FROM land_transaction '
        'WHERE county_city IS NOT NULL AND county_city != ""'
    )
    has_city = cur.fetchone()[0]
    cur.execute(
        'SELECT COUNT(*) FROM land_transaction '
        'WHERE lat IS NOT NULL AND lat != 0'
    )
    has_geo = cur.fetchone()[0]
    cur.execute(
        'SELECT COUNT(*) FROM land_transaction '
        'WHERE community_name IS NOT NULL AND community_name != ""'
    )
    has_comm = cur.fetchone()[0]
    cur.execute(
        'SELECT COUNT(*) FROM land_transaction '
        'WHERE street IS NOT NULL AND street != ""'
    )
    has_street = cur.fetchone()[0]
    conn.close()

    elapsed = time.time() - t0
    db_size = os.path.getsize(output_path) / 1024 / 1024

    def pct(n):
        return n / total * 100 if total else 0

    print(f'\nğŸ‰ è½‰æ›å®Œæˆ!')
    print(f'  ç¸½ç­†æ•¸:         {total:,}')
    print(f'  æœ‰ç¸£å¸‚å:       {has_city:,} ({pct(has_city):.1f}%)')
    print(f'  åœ°å€è§£ææˆåŠŸ:   {has_street:,} ({pct(has_street):.1f}%)')
    print(f'  æœ‰ç¶“ç·¯åº¦:       {has_geo:,} ({pct(has_geo):.1f}%)')
    print(f'  æœ‰ç¤¾å€å:       {has_comm:,} ({pct(has_comm):.1f}%)')
    print(f'  è€—æ™‚:           {elapsed:.1f} ç§’')
    print(f'  è³‡æ–™åº«å¤§å°:     {db_size:.1f} MB')


def main():
    parser = argparse.ArgumentParser(
        description='å°ç£å¯¦åƒ¹ç™»éŒ„è³‡æ–™è½‰æ› v3 â€” æ”¯æ´ CSV / API / åˆä½µ',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""ç¯„ä¾‹:
  python3 convert.py                              # é è¨­: both
  python3 convert.py --source csv                 # åƒ… CSV
  python3 convert.py --source api                 # åƒ… API DB
  python3 convert.py --source both                # CSV + API åˆä½µ
  python3 convert.py --csv-input a.csv --api-input t.db -o out.db
        """
    )
    parser.add_argument('--source', '-s',
                        choices=['csv', 'api', 'both'], default='both',
                        help='è³‡æ–™ä¾†æºæ¨¡å¼ (é è¨­: both)')
    parser.add_argument('--csv-input', default=None,
                        help='CSV è¼¸å…¥è·¯å¾‘ (é è¨­: ../db/ALL_lvr_land_a.csv)')
    parser.add_argument('--api-input', default=None,
                        help='API DB è·¯å¾‘ (é è¨­: ../db/transactions_all_original.db)')
    parser.add_argument('--output', '-o', default=None,
                        help='SQLite è¼¸å‡ºè·¯å¾‘ (é è¨­: ../db/land_data.db)')
    args = parser.parse_args()

    script_dir = os.path.dirname(os.path.abspath(__file__))
    project_dir = os.path.dirname(script_dir)

    csv_path = args.csv_input or os.path.join(
        project_dir, 'db', 'ALL_lvr_land_a.csv')
    api_path = args.api_input or os.path.join(
        project_dir, 'db', 'transactions_all_original.db')
    output_path = args.output or os.path.join(
        project_dir, 'db', 'land_data.db')

    # é©—è­‰è¼¸å…¥ä¾†æº
    if args.source in ('csv', 'both'):
        if not os.path.exists(csv_path):
            print(f'âŒ æ‰¾ä¸åˆ° CSV æª”æ¡ˆ: {csv_path}')
            sys.exit(1)
    if args.source in ('api', 'both'):
        if not os.path.exists(api_path):
            print(f'âŒ æ‰¾ä¸åˆ° API DB: {api_path}')
            sys.exit(1)

    convert(args.source, csv_path, api_path, output_path)


if __name__ == '__main__':
    main()
