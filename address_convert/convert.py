#!/usr/bin/env python3
"""
å°ç£ä¸å‹•ç”¢å¯¦åƒ¹ç™»éŒ„è³‡æ–™è½‰æ›è…³æœ¬ v4

æ ¸å¿ƒåŠŸèƒ½:
  1. è‡ªå‹•è­˜åˆ¥è¼¸å…¥è³‡æ–™ä¾†æº (CSV / API DB / å…¶ä»– .db)
  2. æ¸…æ´—ã€æ­£è¦åŒ–ã€çµæ§‹åŒ–
  3. å¢é‡åŒ¯å…¥ land_data.db (å»é‡ + enrich + æ–°å¢)

ç”¨æ³•:
  python3 convert.py <input_file>                    # è‡ªå‹•è­˜åˆ¥
  python3 convert.py data.csv                        # CSV åŒ¯å…¥
  python3 convert.py transactions.db                 # API DB åŒ¯å…¥
  python3 convert.py a.csv b.db c.csv                # å¤šæª”ä¾åºåŒ¯å…¥
  python3 convert.py --rebuild a.csv b.db            # é‡å»º land_data.db
  python3 convert.py --target /path/to/land_data.db  # æŒ‡å®šç›®æ¨™ DB

å»é‡ç­–ç•¥:
  ä»¥ (äº¤æ˜“æ—¥æœŸå‰7ç¢¼ + æ­£è¦åŒ–åœ°å€ + ç¸½åƒ¹) ä¸‰éµåˆ¤æ–·æ˜¯å¦ç‚ºåŒä¸€ç­†äº¤æ˜“ã€‚
  åŒä¸€å¤©ã€åŒåœ°å€ä½†ä¸åŒåƒ¹æ ¼è¦–ç‚ºä¸åŒäº¤æ˜“ã€‚
    - å·²å­˜åœ¨ä¸”æ–°è³‡æ–™æœ‰é¡å¤–æ¬„ä½ â†’ enrich (è£œå……)
    - ä¸å­˜åœ¨ â†’ æ–°å¢
    - è³‡æ–™ç¼ºæ (ç„¡åœ°å€/ç„¡è™Ÿ) â†’ ä¸Ÿæ£„
"""

import csv
import json
import sqlite3
import os
import sys
import argparse
import re
import time
import hashlib
import math
from enum import Enum
from typing import Optional, Dict, List, Tuple, Any

# â”€â”€ å…±ç”¨æ¨¡çµ„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
sys.path.insert(0, os.path.join(os.path.dirname(os.path.abspath(__file__)), '..'))

# å…¨åŸŸ verbose æ——æ¨™ (ç”± main() è¨­å®š)
_VERBOSE = False
_VERBOSE_MAX = float('inf')  # ä¸é™åˆ¶ï¼šæ‰€æœ‰ç¯„ä¾‹éƒ½å°å‡ºä¸¦å¯«å…¥ log

# æ—¥èªŒæª”æ¡ˆå¥æŸ„èˆ‡å‡½å¼
_LOG_FILE = None

def log_print(*args, **kwargs):
    """åŒæ™‚è¼¸å‡ºåˆ° stdout å’Œæ—¥èªŒæª”æ¡ˆ"""
    msg = ' '.join(str(a) for a in args)
    print(*args, **kwargs)
    if _LOG_FILE:
        print(msg, file=_LOG_FILE, flush=True)

def init_logging(log_path: str):
    """åˆå§‹åŒ–æ—¥èªŒæª”æ¡ˆ"""
    global _LOG_FILE
    try:
        _LOG_FILE = open(log_path, 'w', encoding='utf-8', buffering=1)
        log_print(f'[{time.strftime("%Y-%m-%d %H:%M:%S")}] é–‹å§‹åŒ¯å…¥')
    except Exception as e:
        print(f'âš ï¸ ç„¡æ³•é–‹å•Ÿæ—¥èªŒæª”æ¡ˆ: {e}', flush=True)
        _LOG_FILE = None

def close_logging():
    """é—œé–‰æ—¥èªŒæª”æ¡ˆ"""
    global _LOG_FILE
    if _LOG_FILE:
        log_print(f'[{time.strftime("%Y-%m-%d %H:%M:%S")}] åŒ¯å…¥å®Œæˆ')
        _LOG_FILE.close()
        _LOG_FILE = None

from address_utils import (
    normalize_address,
    parse_address,
    chinese_numeral_to_int,
    fullwidth_to_halfwidth,
    CITY_CODE_MAP,
    AMBIGUOUS_DISTRICTS,
    DISTRICT_CITY_MAP,
)

# å‘å¾Œç›¸å®¹åˆ¥å (ä¾› test_convert.py ç­‰ä½¿ç”¨)
normalize_address_numbers = normalize_address


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ç¬¬ä¸€å±¤: å®‰å…¨å‹åˆ¥è½‰æ›
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def safe_int(val, default=None):
    if val is None or val == '':
        return default
    if isinstance(val, int):
        return val
    try:
        # å¿«é€Ÿè·¯å¾‘: ç´”æ•¸å­—å­—ä¸²
        return int(val)
    except (ValueError, TypeError):
        try:
            return int(float(str(val).replace(',', '').replace(' ', '')))
        except (ValueError, TypeError):
            return default


def safe_float(val, default=None):
    if val is None or val == '':
        return default
    if isinstance(val, (int, float)):
        return float(val)
    try:
        return float(val)
    except (ValueError, TypeError):
        try:
            return float(str(val).replace(',', ''))
        except (ValueError, TypeError):
            return default


def parse_price(val):
    """'39,380,000' â†’ int"""
    if not val:
        return None
    try:
        return int(str(val).replace(',', '').replace(' ', ''))
    except Exception:
        return None


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ç¬¬äºŒå±¤: è³‡æ–™ä¾†æºè‡ªå‹•è­˜åˆ¥
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class SourceType(Enum):
    """è³‡æ–™ä¾†æºé¡å‹"""
    CSV_LVR = 'csv_lvr'        # æ”¿åºœå¯¦åƒ¹ç™»éŒ„ CSV (33 æ¬„, é›™è¡Œè¡¨é ­)
    CSV_GENERIC = 'csv_generic' # å…¶ä»– CSV (å˜—è©¦æ¬„ä½åæ˜ å°„)
    API_DB = 'api_db'          # transactions.db (LVR API æŠ“å–)
    LAND_DB = 'land_db'        # å·²å­˜åœ¨çš„ land_data.db
    UNKNOWN = 'unknown'


# LVR CSV çš„å·²çŸ¥ä¸­æ–‡æ¨™é ­é—œéµå­— (å‰8æ¬„)
_LVR_CSV_KEYWORDS = {'é„‰é®å¸‚å€', 'äº¤æ˜“æ¨™çš„', 'åœŸåœ°ä½ç½®å»ºç‰©é–€ç‰Œ', 'äº¤æ˜“å¹´æœˆæ—¥', 'ç¸½åƒ¹å…ƒ'}


def detect_source(filepath: str) -> SourceType:
    """
    è‡ªå‹•åµæ¸¬è¼¸å…¥ä¾†æºé¡å‹ã€‚

    åµæ¸¬é‚è¼¯:
      .csv  â†’ è®€å‰2è¡Œæ¨™é ­ â†’ LVR CSV or generic CSV
      .db   â†’ æŸ¥ schema â†’ transactions è¡¨ â†’ API_DB
                        â†’ land_transaction è¡¨ â†’ LAND_DB
      å…¶ä»–  â†’ UNKNOWN
    """
    ext = os.path.splitext(filepath)[1].lower()

    if ext == '.csv':
        return _detect_csv_type(filepath)
    elif ext in ('.db', '.sqlite', '.sqlite3'):
        return _detect_db_type(filepath)
    else:
        # å˜—è©¦ç•¶ CSV è®€
        try:
            return _detect_csv_type(filepath)
        except Exception:
            return SourceType.UNKNOWN


def _detect_csv_type(filepath: str) -> SourceType:
    """åµæ¸¬ CSV å­é¡å‹"""
    try:
        with open(filepath, 'r', encoding='utf-8-sig') as f:
            first_line = f.readline().strip()
    except Exception:
        return SourceType.UNKNOWN

    # ç”¨é€—è™Ÿæ‹†é–‹çœ‹æœ‰æ²’æœ‰ LVR é—œéµå­—
    fields = set(first_line.split(','))
    if fields & _LVR_CSV_KEYWORDS:
        return SourceType.CSV_LVR

    # çœ‹æœ‰æ²’æœ‰å…¶ä»–å¯è¾¨è­˜çš„æ¬„ä½
    known_cols = {'address', 'åœ°å€', 'total_price', 'ç¸½åƒ¹', 'transaction_date', 'äº¤æ˜“æ—¥æœŸ'}
    if fields & known_cols:
        return SourceType.CSV_GENERIC

    # fallback: å¦‚æœæ¬„æ•¸ >= 28 ä¸”ç¬¬ä¸€è¡Œçœ‹èµ·ä¾†åƒä¸­æ–‡æ¨™é ­ï¼Œè¦–ç‚º LVR
    if len(fields) >= 28:
        return SourceType.CSV_LVR

    return SourceType.CSV_GENERIC


def _detect_db_type(filepath: str) -> SourceType:
    """åµæ¸¬ SQLite DB å­é¡å‹"""
    try:
        conn = sqlite3.connect(filepath)
        cur = conn.cursor()
        cur.execute("SELECT name FROM sqlite_master WHERE type='table'")
        tables = {row[0] for row in cur.fetchall()}
        conn.close()
    except Exception:
        return SourceType.UNKNOWN

    if 'land_transaction' in tables:
        return SourceType.LAND_DB
    if 'transactions' in tables:
        return SourceType.API_DB

    return SourceType.UNKNOWN


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ç¬¬ä¸‰å±¤: åœ°å€/æ—¥æœŸ/æ¨“å±¤ å·¥å…·å‡½å¼
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

CHINESE_FLOOR = {
    'ä¸€': 1, 'äºŒ': 2, 'ä¸‰': 3, 'å››': 4, 'äº”': 5, 'å…­': 6, 'ä¸ƒ': 7,
    'å…«': 8, 'ä¹': 9, 'å': 10, 'åä¸€': 11, 'åäºŒ': 12, 'åä¸‰': 13,
    'åå››': 14, 'åäº”': 15, 'åå…­': 16, 'åä¸ƒ': 17, 'åå…«': 18,
    'åä¹': 19, 'äºŒå': 20, 'äºŒåä¸€': 21, 'äºŒåäºŒ': 22, 'äºŒåä¸‰': 23,
    'äºŒåå››': 24, 'äºŒåäº”': 25, 'äºŒåå…­': 26, 'äºŒåä¸ƒ': 27,
    'äºŒåå…«': 28, 'äºŒåä¹': 29, 'ä¸‰å': 30,
    'åœ°ä¸‹ä¸€': -1, 'åœ°ä¸‹äºŒ': -2, 'åœ°ä¸‹ä¸‰': -3,
}


def parse_floor_info(floor_str):
    """è§£ææ¨“å±¤æ¬„ä½: 'ä¹å±¤/åäº”å±¤' â†’ ('9', '15')"""
    if not floor_str:
        return '', ''
    parts = floor_str.split('/')
    if len(parts) == 2:
        fl, tf = parts[0].strip(), parts[1].strip()
        for s, attr in ((fl, 'fl'), (tf, 'tf')):
            stripped = s.rstrip('å±¤')
            if stripped in CHINESE_FLOOR:
                if attr == 'fl':
                    fl = str(CHINESE_FLOOR[stripped])
                else:
                    tf = str(CHINESE_FLOOR[stripped])
        return fl, tf
    return floor_str.strip(), ''


def normalize_date(date_str):
    """'101/01/09' â†’ '1010109'"""
    if not date_str:
        return ''
    return date_str.replace('/', '')


def clean_trans_addr(addr_raw):
    """å– transactions.db åœ°å€ '#' å¾ŒåŠéƒ¨çš„ä¹¾æ·¨åœ°å€"""
    if addr_raw and '#' in addr_raw:
        return addr_raw.split('#', 1)[1]
    return addr_raw or ''


def norm_addr_simple(addr):
    """ç°¡å–®æ­£è¦åŒ–: å…¨å½¢â†’åŠå½¢ã€è‡ºâ†’å°ã€å»ç©ºç™½"""
    return fullwidth_to_halfwidth(addr or '').replace('è‡º', 'å°').replace(' ', '')


def strip_city(addr):
    """ç§»é™¤åœ°å€é–‹é ­çš„ç¸£å¸‚å"""
    for city in CITY_CODE_MAP.values():
        if addr.startswith(city):
            return addr[len(city):]
    for old in ('å°åŒ—ç¸£', 'æ¡ƒåœ’ç¸£', 'å°ä¸­ç¸£', 'å°å—ç¸£', 'é«˜é›„ç¸£'):
        if addr.startswith(old):
            return addr[len(old):]
    return addr


def strip_floor(addr):
    """å»é™¤å°¾ç«¯æ¨“å±¤è³‡è¨Šï¼Œå–å¾—å»ºç‰©åŸºç¤åœ°å€"""
    addr = re.sub(r'(-\d+|åœ°ä¸‹\d+|\d+)æ¨“[ä¹‹\d]*$', '', addr)
    return addr.rstrip('ä¹‹å·è™Ÿ ')


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ç¬¬å››å±¤: land_data.db ç®¡ç† (schema + å»é‡ + enrich)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# â€”â€” land_transaction æ‰€æœ‰æ¬„ä½å (ä¸å« id) â€”â€”
LAND_COLUMNS = [
    'raw_district', 'transaction_type', 'address', 'land_area',
    'urban_zone', 'non_urban_zone', 'non_urban_use',
    'transaction_date', 'transaction_count', 'floor_level', 'total_floors',
    'building_type', 'main_use', 'main_material', 'build_date',
    'building_area', 'rooms', 'halls', 'bathrooms', 'partitioned',
    'has_management', 'total_price', 'unit_price',
    'parking_type', 'parking_area', 'parking_price',
    'note', 'serial_no', 'main_area', 'attached_area', 'balcony_area',
    'elevator', 'transfer_no',
    'county_city', 'district', 'village', 'street', 'lane', 'alley',
    'number', 'floor', 'sub_number',
    'community_name', 'lat', 'lng',
]

INSERT_SQL = (
    'INSERT INTO land_transaction ('
    + ', '.join(LAND_COLUMNS)
    + ') VALUES ('
    + ', '.join(['?'] * len(LAND_COLUMNS))
    + ')'
)

# åˆ¤å®šã€Œç©ºã€çš„æ¬„ä½å â†’ åˆ¤ç©ºå‡½æ•¸
_EMPTY_NUMERIC = lambda v: v is None or v == 0
_EMPTY_TEXT = lambda v: not v

# å“ªäº›æ¬„ä½åœ¨æ¯”è¼ƒæ™‚ç®—ã€Œæœ‰è³‡è¨Šã€ï¼ˆç”¨æ–¼ enrich åˆ¤æ–·ï¼‰
ENRICH_FIELDS = [
    ('lat',              _EMPTY_NUMERIC),
    ('lng',              _EMPTY_NUMERIC),
    ('community_name',   _EMPTY_TEXT),
    ('county_city',      _EMPTY_TEXT),
    ('building_type',    _EMPTY_TEXT),
    ('main_use',         _EMPTY_TEXT),
    ('main_material',    _EMPTY_TEXT),
    ('has_management',   _EMPTY_TEXT),
    ('rooms',            lambda v: v is None),
    ('halls',            lambda v: v is None),
    ('bathrooms',        lambda v: v is None),
    ('building_area',    _EMPTY_NUMERIC),
    ('unit_price',       _EMPTY_NUMERIC),
    ('transaction_type', _EMPTY_TEXT),
    ('floor_level',      _EMPTY_TEXT),
    ('total_floors',     _EMPTY_TEXT),
    ('note',             _EMPTY_TEXT),
    ('land_area',        _EMPTY_NUMERIC),
    ('urban_zone',       _EMPTY_TEXT),
    ('parking_type',     _EMPTY_TEXT),
    ('parking_area',     _EMPTY_NUMERIC),
    ('parking_price',    _EMPTY_NUMERIC),
    ('main_area',        _EMPTY_NUMERIC),
    ('attached_area',    _EMPTY_NUMERIC),
    ('balcony_area',     _EMPTY_NUMERIC),
    ('elevator',         _EMPTY_TEXT),
]

INSERT_DEDUP_SQL = (
    'INSERT INTO land_transaction ('
    + ', '.join(LAND_COLUMNS + ['dedup_key'])
    + ') VALUES ('
    + ', '.join(['?'] * (len(LAND_COLUMNS) + 1))
    + ')'
)


class _BloomFilter:
    """Compact bloom filter for dedup key existence checking.

    For 5M items at 0.1% false-positive rate:
      - size â‰ˆ 72M bits â‰ˆ 9 MB
      - num_hashes â‰ˆ 10
    Memory is O(1) regardless of item count (fixed-size bytearray).
    """
    __slots__ = ('size', 'num_hashes', 'bits')

    def __init__(self, expected_items: int = 5_000_000, fp_rate: float = 0.001):
        self.size = int(-expected_items * math.log(fp_rate) / (math.log(2) ** 2))
        self.num_hashes = max(1, int((self.size / expected_items) * math.log(2)))
        self.bits = bytearray((self.size + 7) // 8)

    def _hashes(self, key: str):
        h = hashlib.md5(key.encode('utf-8')).digest()
        h1 = int.from_bytes(h[:8], 'little')
        h2 = int.from_bytes(h[8:], 'little')
        size = self.size
        for i in range(self.num_hashes):
            yield (h1 + i * h2) % size

    def add(self, key: str):
        bits = self.bits
        for pos in self._hashes(key):
            bits[pos >> 3] |= (1 << (pos & 7))

    def __contains__(self, key: str) -> bool:
        bits = self.bits
        return all(bits[pos >> 3] & (1 << (pos & 7)) for pos in self._hashes(key))

    def memory_mb(self) -> float:
        return len(self.bits) / 1024 / 1024


class LandDataDB:
    """
    ç®¡ç† land_data.db çš„è®€å¯«ã€å»é‡èˆ‡ enrichã€‚

    ä½¿ç”¨æ–¹å¼:
        db = LandDataDB('/path/to/land_data.db')
        db.open()
        db.upsert_record(record_dict)   # è‡ªå‹•å»é‡ + enrich
        db.finalize()                    # å»ºç´¢å¼• + FTS + VACUUM
        db.close()
    """

    def __init__(self, db_path: str):
        self.db_path = db_path
        self.conn: Optional[sqlite3.Connection] = None
        self._bloom = _BloomFilter(expected_items=5_000_000, fp_rate=0.001)
        self._batch_keys: set = set()  # ç•¶å‰æ‰¹æ¬¡çš„ dedup_key (bounded to BATCH_SIZE)
        self._insert_batch: list = []
        self._enrich_batch: list = []
        self._init_stats()
        self.BATCH_SIZE = 50000

    def _init_stats(self):
        self._stats = {
            'inserted': 0, 'enriched': 0,
            'duplicated': 0, 'discarded': 0, 'total_scanned': 0,
            'discard_no_addr': 0,
            'discard_no_number': 0,
            'discard_parse_err': 0,
        }
        self._verbose_count = {'discarded': 0, 'enriched': 0, 'duplicated': 0}

    def open(self, rebuild=False, load_dedup=True):
        """
        é–‹å•Ÿ (æˆ–å»ºç«‹) land_data.dbã€‚
        rebuild=True æ™‚æœƒåˆªé™¤èˆŠ DB é‡å»ºã€‚
        load_dedup=False æ™‚è·³éå»é‡éµè¼‰å…¥ï¼ˆåƒ…åš backfill æ™‚ä½¿ç”¨ï¼‰ã€‚
        """
        if rebuild and os.path.exists(self.db_path):
            os.remove(self.db_path)
            log_print(f'  ğŸ—‘  å·²åˆªé™¤èˆŠè³‡æ–™åº«: {self.db_path}')

        is_new = not os.path.exists(self.db_path)
        self.conn = sqlite3.connect(self.db_path)
        cur = self.conn.cursor()

        # æ‰¹é‡åŒ¯å…¥æ•ˆèƒ½è¨­å®š (finalize æ™‚æœƒæ¢å¾©)
        cur.execute('PRAGMA journal_mode=WAL')
        cur.execute('PRAGMA synchronous=OFF')        # åŒ¯å…¥æœŸé–“é—œé–‰åŒæ­¥ (finalize æ¢å¾©)
        cur.execute('PRAGMA cache_size=-256000')      # 256MB cache
        cur.execute('PRAGMA temp_store=MEMORY')
        cur.execute('PRAGMA locking_mode=EXCLUSIVE')  # ç¨ä½”é–å®šé¿å…é–é–‹éŠ·
        cur.execute('PRAGMA page_size=8192')           # è¼ƒå¤§é é¢æå‡å¤§è¡¨æ•ˆèƒ½

        self._create_tables(cur)
        cur.execute('CREATE INDEX IF NOT EXISTS idx_dedup_key ON land_transaction(dedup_key)')
        self.conn.commit()

        if is_new:
            log_print(f'  âœ¨ å»ºç«‹æ–°è³‡æ–™åº«: {self.db_path}')
        else:
            count = cur.execute('SELECT COUNT(*) FROM land_transaction').fetchone()[0]
            log_print(f'  ğŸ“‚ é–‹å•Ÿæ—¢æœ‰è³‡æ–™åº«: {self.db_path} ({count:,} ç­†)')
            # å¢é‡åŒ¯å…¥æ™‚ï¼Œå…ˆæš«æ™‚ç§»é™¤éå¿…è¦ç´¢å¼•ä»¥åŠ é€Ÿå¯«å…¥
            self._drop_non_essential_indexes(cur)

        # è¼‰å…¥å»é‡éµå€¼
        if load_dedup:
            self._load_dedup_keys()

    def _create_tables(self, cursor):
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS land_transaction (
                id              INTEGER PRIMARY KEY AUTOINCREMENT,
                raw_district    TEXT,
                transaction_type TEXT,
                address         TEXT,
                land_area       REAL,
                urban_zone      TEXT,
                non_urban_zone  TEXT,
                non_urban_use   TEXT,
                transaction_date TEXT,
                transaction_count TEXT,
                floor_level     TEXT,
                total_floors    TEXT,
                building_type   TEXT,
                main_use        TEXT,
                main_material   TEXT,
                build_date      TEXT,
                building_area   REAL,
                rooms           INTEGER,
                halls           INTEGER,
                bathrooms       INTEGER,
                partitioned     TEXT,
                has_management  TEXT,
                total_price     INTEGER,
                unit_price      REAL,
                parking_type    TEXT,
                parking_area    REAL,
                parking_price   INTEGER,
                note            TEXT,
                serial_no       TEXT,
                main_area       REAL,
                attached_area   REAL,
                balcony_area    REAL,
                elevator        TEXT,
                transfer_no     TEXT,
                county_city     TEXT,
                district        TEXT,
                village         TEXT,
                street          TEXT,
                lane            TEXT,
                alley           TEXT,
                number          TEXT,
                floor           TEXT,
                sub_number      TEXT,
                community_name  TEXT,
                lat             REAL,
                lng             REAL,
                dedup_key       TEXT
            )
        ''')

    def _load_dedup_keys(self):
        """å¾æ—¢æœ‰è³‡æ–™è¼‰å…¥ dedup_key åˆ° Bloom filter (~9 MB)"""
        cur = self.conn.cursor()
        # æª¢æŸ¥æ˜¯å¦æœ‰ dedup_key æ¬„ä½ (å‘å¾Œç›¸å®¹)
        cur.execute('PRAGMA table_info(land_transaction)')
        cols = {row[1] for row in cur.fetchall()}
        if 'dedup_key' not in cols:
            log_print('    âš  èˆŠç‰ˆ DB ç„¡ dedup_key æ¬„ä½ï¼Œè·³éè¼‰å…¥')
            return

        cur.execute('SELECT dedup_key FROM land_transaction WHERE dedup_key IS NOT NULL')
        count = 0
        for (key,) in cur:
            self._bloom.add(key)
            count += 1
        log_print(f'    Bloom filter: {count:,} æ—¢æœ‰éµå€¼ (~{self._bloom.memory_mb():.1f} MB)')

    def _drop_non_essential_indexes(self, cursor):
        """æš«æ™‚ç§»é™¤éå»é‡ç´¢å¼•ï¼Œå¤§å¹…åŠ é€Ÿæ‰¹é‡å¯«å…¥"""
        # ä¿ç•™ idx_dedup_key (å»é‡å¿…éœ€)ï¼Œå…¶é¤˜åœ¨ finalize() é‡å»º
        drop_indexes = [
            'idx_county_city', 'idx_district', 'idx_street', 'idx_lane',
            'idx_number', 'idx_floor', 'idx_date', 'idx_price', 'idx_serial',
            'idx_community',
            'idx_addr_combo', 'idx_community_address', 'idx_street_lane_district',
            'idx_search_numbers', 'idx_district_street_number',
            'idx_district_street_lane', 'idx_community_district',
        ]
        dropped = 0
        for idx_name in drop_indexes:
            try:
                cursor.execute(f'DROP INDEX IF EXISTS {idx_name}')
                dropped += 1
            except Exception:
                pass
        if dropped:
            self.conn.commit()
            log_print(f'    ğŸ—‘  æš«ç§» {dropped} å€‹ç´¢å¼• (finalize æ™‚é‡å»º)')

    def upsert_record(self, rec: dict):
        """
        æ™ºæ…§åŒ¯å…¥ä¸€ç­†è¨˜éŒ„ã€‚

        é‚è¼¯:
          1. æª¢é©—è³‡æ–™å“è³ª â†’ ä¸åˆæ ¼ â†’ discard
          2. è¨ˆç®— dedup_key = "date7|addr_norm|price"
          3. æª¢æŸ¥ batch_keys â†’ bloom filter â†’ DB
          4. å·²å­˜åœ¨ â†’ enrich (è£œå……ç©ºæ¬„ä½) æˆ– duplicate
          5. ä¸å­˜åœ¨ â†’ insert
        """
        self._stats['total_scanned'] += 1

        # â€”â€” è³‡æ–™å“è³ªé©—è­‰ â€”â€”
        addr = rec.get('address', '')
        if not addr:
            self._stats['discarded'] += 1
            self._stats['discard_no_addr'] += 1
            return
        if not re.search(r'è™Ÿ|åœ°è™Ÿ', addr):
            self._stats['discarded'] += 1
            self._stats['discard_no_number'] += 1
            if _VERBOSE and self._verbose_count['discarded'] < _VERBOSE_MAX:
                log_print(f'    [ä¸Ÿæ£„] ç„¡è™Ÿ: {addr}')
                self._verbose_count['discarded'] += 1
            return

        # â€”â€” è¨ˆç®— dedup key (ä¸‰éµ: æ—¥æœŸ + åœ°å€ + ç¸½åƒ¹) â€”â€”
        date_str = rec.get('transaction_date', '') or ''
        d = date_str.replace('/', '')[:7]
        addr_norm = strip_city(norm_addr_simple(addr))
        price = rec.get('total_price') or 0
        try:
            price = int(price)
        except (ValueError, TypeError):
            price = 0

        if not addr_norm:
            # ç„¡æ³•æ­£è¦åŒ–åœ°å€ â†’ ç›´æ¥æ’å…¥ (ä¸åšå»é‡)
            values = tuple(rec.get(col) for col in LAND_COLUMNS)
            self._insert_batch.append((*values, None))
            self._stats['inserted'] += 1
            if len(self._insert_batch) >= self.BATCH_SIZE:
                self._flush_inserts()
            return

        dedup_key = f"{d}|{addr_norm}|{price}"

        # â€”â€” Level 1: æª¢æŸ¥ç•¶å‰æ‰¹æ¬¡ (O(1), set æœ€å¤š BATCH_SIZE å€‹) â€”â€”
        if dedup_key in self._batch_keys:
            self._stats['duplicated'] += 1
            if _VERBOSE and self._verbose_count['duplicated'] < _VERBOSE_MAX:
                log_print(f'    [é‡è¤‡-batch] serial={rec.get("serial_no","?")} key={dedup_key}: {addr}')
                self._verbose_count['duplicated'] += 1
            return

        # â€”â€” Level 2: æª¢æŸ¥ Bloom filter (~9 MB, O(k)) â€”â€”
        if dedup_key in self._bloom:
            # Bloom filter hit â†’ å¯èƒ½æ˜¯é‡è¤‡ï¼ŒæŸ¥ DB ç¢ºèª (0.1% å½é™½æ€§)
            row = self.conn.execute(
                'SELECT id FROM land_transaction WHERE dedup_key = ?',
                (dedup_key,)
            ).fetchone()
            if row:
                existing_id = row[0]
                enriched = self._try_enrich(existing_id, rec)
                if enriched:
                    self._stats['enriched'] += 1
                    if _VERBOSE and self._verbose_count['enriched'] < _VERBOSE_MAX:
                        detail = ', '.join(f'{k}={v}' for k, v in enriched.items())
                        log_print(f'    [è£œå……] exist_id={existing_id} serial={rec.get("serial_no","?")} {detail}: {addr}')
                        self._verbose_count['enriched'] += 1
                else:
                    self._stats['duplicated'] += 1
                    if _VERBOSE and self._verbose_count['duplicated'] < _VERBOSE_MAX:
                        log_print(f'    [é‡è¤‡] exist_id={existing_id} serial={rec.get("serial_no","?")} key={dedup_key}: {addr}')
                        self._verbose_count['duplicated'] += 1
                return
            # Bloom false positive â†’ fall through to insert

        # â€”â€” æ–°è¨˜éŒ„ â†’ æ’å…¥ â€”â€”
        values = tuple(rec.get(col) for col in LAND_COLUMNS)
        self._insert_batch.append((*values, dedup_key))
        self._batch_keys.add(dedup_key)
        self._bloom.add(dedup_key)
        self._stats['inserted'] += 1

        if len(self._insert_batch) >= self.BATCH_SIZE:
            self._flush_inserts()

    def _try_enrich(self, row_id: int, new_rec: dict) -> list:
        """
        å˜—è©¦ç”¨æ–°è³‡æ–™è£œå……æ—¢æœ‰è¨˜éŒ„çš„ç©ºæ¬„ä½ã€‚
        å›å‚³è£œå……çš„æ¬„ä½ååˆ—è¡¨ (ç©ºåˆ—è¡¨=æ²’æ›´æ–°)ã€‚
        """
        # è®€å–æ—¢æœ‰æ¬„ä½
        cols_to_check = [col for col, _ in ENRICH_FIELDS]
        col_sql = ', '.join(cols_to_check)
        cur = self.conn.cursor()
        row = cur.execute(
            f'SELECT {col_sql} FROM land_transaction WHERE id = ?',
            (row_id,)
        ).fetchone()
        if not row:
            return False

        updates = {}
        for i, (col_name, is_empty) in enumerate(ENRICH_FIELDS):
            current_val = row[i]
            if is_empty(current_val):
                new_val = new_rec.get(col_name)
                if new_val is not None and new_val != '' and new_val != 0:
                    updates[col_name] = new_val

        if not updates:
            return []

        self._enrich_batch.append((updates, row_id))
        if len(self._enrich_batch) >= self.BATCH_SIZE:
            self._flush_enriches()
        return updates

    def _flush_inserts(self):
        if not self._insert_batch:
            return
        self.conn.executemany(INSERT_DEDUP_SQL, self._insert_batch)
        self.conn.commit()
        self._insert_batch = []
        self._batch_keys.clear()

    def _flush_enriches(self):
        if not self._enrich_batch:
            return
        for updates, row_id in self._enrich_batch:
            set_clauses = ', '.join(f'{col} = ?' for col in updates)
            values = list(updates.values()) + [row_id]
            self.conn.execute(
                f'UPDATE land_transaction SET {set_clauses} WHERE id = ?',
                values
            )
        self.conn.commit()
        self._enrich_batch = []

    def flush_all(self):
        """å¼·åˆ¶å¯«å…¥æ‰€æœ‰å¾…è™•ç†æ‰¹æ¬¡"""
        self._flush_inserts()
        self._flush_enriches()

    def fast_insert_records(self, records):
        """
        æ‰¹æ¬¡å¿«é€Ÿæ’å…¥ (è·³éé€ç­† upsert çš„ Python é–‹éŠ·)ã€‚

        é©ç”¨æ–¼: rebuild æ¨¡å¼æˆ–ç¢ºèªç„¡éœ€ enrich çš„å ´æ™¯ã€‚
        é‚è¼¯:
          1. ä½¿ç”¨é è¨ˆç®—çš„ _dedup_key (è‹¥æœ‰)
          2. æ‰¹æ¬¡ bloom filter æª¢æŸ¥ (Python set å»é‡åŒæ‰¹é‡è¤‡)
          3. ä¸€æ¬¡ executemany æ’å…¥

        æ¯” upsert_record å¿« 3-4 å€ (æ¸›å°‘ per-record Python é–‹éŠ·)ã€‚
        """
        batch_insert = []
        _norm = norm_addr_simple
        _strip = strip_city
        _bloom = self._bloom
        _batch_keys = self._batch_keys
        stats = self._stats

        for rec in records:
            stats['total_scanned'] += 1

            addr = rec.get('address', '')
            if not addr:
                stats['discarded'] += 1
                stats['discard_no_addr'] += 1
                continue
            if 'è™Ÿ' not in addr and 'åœ°è™Ÿ' not in addr:
                stats['discarded'] += 1
                stats['discard_no_number'] += 1
                continue

            # ä½¿ç”¨é è¨ˆç®—çš„ _dedup_key (è‹¥ parser å·²æä¾›)
            dedup_key = rec.get('_dedup_key')
            if dedup_key is None:
                # fallback: å‹•æ…‹è¨ˆç®—
                date_str = rec.get('transaction_date', '') or ''
                d = date_str.replace('/', '')[:7]
                addr_norm = _strip(_norm(addr))
                price = rec.get('total_price') or 0
                try:
                    price = int(price)
                except (ValueError, TypeError):
                    price = 0
                dedup_key = f"{d}|{addr_norm}|{price}" if addr_norm else None

            if dedup_key:
                # å¿«é€Ÿå»é‡: set + bloom (ä¸æŸ¥ DB)
                if dedup_key in _batch_keys:
                    stats['duplicated'] += 1
                    continue

                if dedup_key in _bloom:
                    stats['duplicated'] += 1
                    continue

                _batch_keys.add(dedup_key)
                _bloom.add(dedup_key)

            values = tuple(rec.get(col) for col in LAND_COLUMNS)
            batch_insert.append((*values, dedup_key))
            stats['inserted'] += 1

        # æ‰¹é‡æ’å…¥
        if batch_insert:
            self.conn.executemany(INSERT_DEDUP_SQL, batch_insert)

        # é¿å… batch_keys ç„¡é™æˆé•·
        if len(self._batch_keys) > 100000:
            self._batch_keys.clear()

    def fast_insert_tuples(self, tuples_list):
        """
        æ¥µé€Ÿæ‰¹æ¬¡æ’å…¥ (ç›´æ¥æ¥æ”¶ tuple åˆ—è¡¨ï¼Œè·³éæ‰€æœ‰ dict é–‹éŠ·)ã€‚

        æ¯å€‹ tuple æ ¼å¼: (*LAND_COLUMNS_values, dedup_key)
        address æ¬„ä½åœ¨ tuple[2]ï¼Œdedup_key åœ¨ tuple[-1]ã€‚
        """
        batch_insert = []
        _bloom = self._bloom
        _batch_keys = self._batch_keys
        stats = self._stats

        for tup in tuples_list:
            stats['total_scanned'] += 1
            addr = tup[2]  # address æ˜¯ç¬¬ 3 å€‹æ¬„ä½

            if not addr:
                stats['discarded'] += 1
                stats['discard_no_addr'] += 1
                continue
            if 'è™Ÿ' not in addr and 'åœ°è™Ÿ' not in addr:
                stats['discarded'] += 1
                stats['discard_no_number'] += 1
                continue

            dedup_key = tup[-1]  # æœ€å¾Œä¸€å€‹æ¬„ä½

            if dedup_key:
                if dedup_key in _batch_keys:
                    stats['duplicated'] += 1
                    continue
                if dedup_key in _bloom:
                    stats['duplicated'] += 1
                    continue
                _batch_keys.add(dedup_key)
                _bloom.add(dedup_key)

            batch_insert.append(tup)
            stats['inserted'] += 1

        if batch_insert:
            self.conn.executemany(INSERT_DEDUP_SQL, batch_insert)

        if len(self._batch_keys) > 100000:
            self._batch_keys.clear()

    def backfill_community(self, api_db_path: str):
        """
        å¾ API DB å›å¡« community_nameã€‚

        æ¼”ç®—æ³•ï¼ˆO(N) å–®æ¬¡æƒæï¼Œä¸ç”¨ LIKEï¼‰:
          Phase 1: å¾ API DB å»ºç«‹ åœ°å€éµå€¼(å»ç¸£å¸‚+å»æ¨“å±¤+åŠå½¢) â†’ community æ˜ å°„
          Phase 2: æƒæ land_transactionï¼Œå°ç„¡ç¤¾å€è¨˜éŒ„åš Python dict æ¯”å°
                   â†’ batch UPDATE
          â€» å…¨å½¢/åŠå½¢åœ°å€çµ±ä¸€åœ¨ Python æ­£è¦åŒ–å¾Œæ¯”å°ï¼Œä¸å†ä¾è³´ SQL LIKE
        """
        if not os.path.exists(api_db_path):
            return 0

        print('  å›å¡«ç¤¾å€å...', flush=True)
        conn_t = sqlite3.connect(api_db_path)
        conn_t.text_factory = lambda b: b.decode('utf-8', errors='replace')
        rows = conn_t.execute(
            "SELECT city, address, community FROM transactions "
            "WHERE community != '' AND community IS NOT NULL AND address != ''"
        ).fetchall()
        conn_t.close()

        # Phase 1: addr_key â†’ {community: vote_count}
        # addr_key = å»ç¸£å¸‚ + å»æ¨“å±¤ + åŠå½¢æ­£è¦åŒ–
        votes: dict = {}
        for _city_code, addr_raw, community in rows:
            addr = strip_floor(strip_city(norm_addr_simple(clean_trans_addr(addr_raw))))
            if not addr or 'è™Ÿ' not in addr:
                continue
            bucket = votes.setdefault(addr, {})
            bucket[community] = bucket.get(community, 0) + 1

        comm_map = {addr: max(v, key=v.get) for addr, v in votes.items()}
        print(f'    ç¤¾å€æ˜ å°„: {len(comm_map):,} å€‹åœ°å€éµå€¼', flush=True)

        # Phase 2: å–®æ¬¡æƒæï¼Œæ¯”å°ç„¡ç¤¾å€çš„è¨˜éŒ„
        cur = self.conn.cursor()
        updates: list = []
        updated = 0

        for row_id, addr in cur.execute(
            "SELECT id, address FROM land_transaction "
            "WHERE community_name IS NULL OR community_name = ''"
        ):
            # å…¨å½¢â†’åŠå½¢æ­£è¦åŒ–å¾Œæ¯”å°ï¼Œè§£æ±º CSV å…¨å½¢èˆ‡ API åŠå½¢ä¸ä¸€è‡´çš„å•é¡Œ
            norm = strip_floor(strip_city(norm_addr_simple(addr or '')))
            community = comm_map.get(norm)
            if community:
                updates.append((community, row_id))
                if len(updates) >= 5000:
                    self.conn.executemany(
                        "UPDATE land_transaction SET community_name = ? WHERE id = ?",
                        updates
                    )
                    self.conn.commit()
                    updated += len(updates)
                    updates = []

        if updates:
            self.conn.executemany(
                "UPDATE land_transaction SET community_name = ? WHERE id = ?",
                updates
            )
            self.conn.commit()
            updated += len(updates)

        return updated

    def finalize(self):
        """å»ºç´¢å¼• + FTS5 + ANALYZE + VACUUMï¼Œä¸¦æ¢å¾©å®‰å…¨çš„ PRAGMA è¨­å®š"""
        self.flush_all()
        cur = self.conn.cursor()

        # æ¢å¾©å®‰å…¨çš„åŒæ­¥è¨­å®š
        cur.execute('PRAGMA synchronous=NORMAL')
        self.conn.commit()

        # å–®æ¬„ç´¢å¼•
        log_print('  ğŸ“‡ å»ºç«‹ç´¢å¼•...')
        indexes = [
            ('idx_county_city', 'county_city'),
            ('idx_district', 'district'),
            ('idx_street', 'street'),
            ('idx_lane', 'lane'),
            ('idx_number', 'number'),
            ('idx_floor', 'floor'),
            ('idx_date', 'transaction_date'),
            ('idx_price', 'total_price'),
            ('idx_serial', 'serial_no'),
            ('idx_dedup_key', 'dedup_key'),
            ('idx_community', 'community_name'),
        ]
        for name, col in indexes:
            cur.execute(f'CREATE INDEX IF NOT EXISTS {name} ON land_transaction({col})')

        # è¤‡åˆç´¢å¼•ï¼ˆåŠ é€ŸæŸ¥è©¢æœå‹™ï¼‰
        composite_indexes = [
            ('idx_addr_combo', 'county_city, district, street, lane, number'),
            ('idx_community_address', 'community_name, address'),
            ('idx_street_lane_district', 'street, lane, district'),
            ('idx_search_numbers', 'street, lane, district, total_floors, build_date'),
            ('idx_district_street_number', 'district, street, number'),
            ('idx_district_street_lane', 'district, street, lane'),
            ('idx_community_district', 'community_name, district'),
        ]
        for name, cols in composite_indexes:
            cur.execute(f'CREATE INDEX IF NOT EXISTS {name} ON land_transaction({cols})')
        self.conn.commit()

        # FTS5
        log_print('  ğŸ” å»ºç«‹ FTS5 å…¨æ–‡æª¢ç´¢...')
        cur.execute('DROP TABLE IF EXISTS address_fts')
        cur.execute('''
            CREATE VIRTUAL TABLE address_fts USING fts5(
                address,
                content='land_transaction',
                content_rowid='id',
                tokenize='unicode61'
            )
        ''')
        cur.execute('''
            INSERT INTO address_fts(rowid, address)
            SELECT id, address FROM land_transaction WHERE address != ''
        ''')
        self.conn.commit()

        # ANALYZE
        log_print('  ğŸ“Š æ›´æ–°çµ±è¨ˆè³‡è¨Š...')
        self.conn.execute('ANALYZE')
        self.conn.commit()

        # VACUUM (éœ€è¦ç´„ç­‰åŒ DB å¤§å°çš„é¡å¤–ç£ç¢Ÿç©ºé–“)
        log_print('  ğŸ—œ  å£“ç¸®è³‡æ–™åº«...')
        try:
            self.conn.execute('PRAGMA journal_mode=DELETE')
            self.conn.commit()
            self.conn.execute('VACUUM')
        except sqlite3.OperationalError as e:
            log_print(f'  âš ï¸  VACUUM å¤±æ•— ({e})ï¼Œè·³éå£“ç¸® (ä¸å½±éŸ¿è³‡æ–™å®Œæ•´æ€§)')
        finally:
            self.conn.execute('PRAGMA journal_mode=WAL')
            self.conn.execute('PRAGMA locking_mode=NORMAL')  # æ¢å¾©æ­£å¸¸é–å®šæ¨¡å¼
            self.conn.execute('PRAGMA synchronous=NORMAL')    # ç¢ºä¿å®‰å…¨åŒæ­¥
            self.conn.commit()

    def print_stats(self):
        """å°å‡ºåŒ¯å…¥çµ±è¨ˆ"""
        s = self._stats
        cur = self.conn.cursor()
        total = cur.execute('SELECT COUNT(*) FROM land_transaction').fetchone()[0]
        has_city = cur.execute(
            'SELECT COUNT(*) FROM land_transaction '
            'WHERE county_city IS NOT NULL AND county_city != ""'
        ).fetchone()[0]
        has_geo = cur.execute(
            'SELECT COUNT(*) FROM land_transaction '
            'WHERE lat IS NOT NULL AND lat != 0'
        ).fetchone()[0]
        has_comm = cur.execute(
            'SELECT COUNT(*) FROM land_transaction '
            'WHERE community_name IS NOT NULL AND community_name != ""'
        ).fetchone()[0]
        has_street = cur.execute(
            'SELECT COUNT(*) FROM land_transaction '
            'WHERE street IS NOT NULL AND street != ""'
        ).fetchone()[0]

        pct = lambda n: n / total * 100 if total else 0
        db_size = os.path.getsize(self.db_path) / 1024 / 1024

        log_print(f'\nğŸ“Š æœ¬æ¬¡åŒ¯å…¥çµ±è¨ˆ:')
        log_print(f'  æƒæ:    {s["total_scanned"]:,}')
        log_print(f'  æ–°å¢:    {s["inserted"]:,}')
        log_print(f'  è£œå……:    {s["enriched"]:,}')
        log_print(f'  é‡è¤‡:    {s["duplicated"]:,}')
        log_print(f'  ä¸Ÿæ£„:    {s["discarded"]:,}'
              + (f'  (ç„¡åœ°å€={s["discard_no_addr"]:,} / ç¼ºè™Ÿ={s["discard_no_number"]:,} / ä¾‹å¤–={s["discard_parse_err"]:,})'
                 if s['discarded'] else ''))
        if _VERBOSE:
            log_print(f'  (verbose æ¨£æœ¬å·²åœ¨ä¸Šæ–¹å³æ™‚è¼¸å‡ºï¼Œå…±å°å‡º: '
                      f'ä¸Ÿæ£„={self._verbose_count["discarded"]} '
                      f'è£œå……={self._verbose_count["enriched"]} '
                      f'é‡è¤‡={self._verbose_count["duplicated"]})')
        
        # æœ€å¾Œé¡¯ç¤ºè³‡æ–™åº«ç¸½è¦½
        log_print(f'\nğŸ“¦ è³‡æ–™åº«ç¸½è¦½:')
        log_print(f'  ç¸½ç­†æ•¸:        {total:,}')
        log_print(f'  æœ‰ç¸£å¸‚å:      {has_city:,} ({pct(has_city):.1f}%)')
        log_print(f'  åœ°å€è§£ææˆåŠŸ:  {has_street:,} ({pct(has_street):.1f}%)')
        log_print(f'  æœ‰ç¶“ç·¯åº¦:      {has_geo:,} ({pct(has_geo):.1f}%)')
        log_print(f'  æœ‰ç¤¾å€å:      {has_comm:,} ({pct(has_comm):.1f}%)')
        log_print(f'  è³‡æ–™åº«å¤§å°:    {db_size:.1f} MB')
        print(f'\nğŸ“¦ è³‡æ–™åº«ç¸½è¦½:')
        print(f'  ç¸½ç­†æ•¸:        {total:,}')
        print(f'  æœ‰ç¸£å¸‚å:      {has_city:,} ({pct(has_city):.1f}%)')
        print(f'  åœ°å€è§£ææˆåŠŸ:  {has_street:,} ({pct(has_street):.1f}%)')
        print(f'  æœ‰ç¶“ç·¯åº¦:      {has_geo:,} ({pct(has_geo):.1f}%)')
        print(f'  æœ‰ç¤¾å€å:      {has_comm:,} ({pct(has_comm):.1f}%)')
        print(f'  è³‡æ–™åº«å¤§å°:    {db_size:.1f} MB')

    def reset_stats(self):
        """é‡ç½®æœ¬æ¬¡çµ±è¨ˆ (å¤šæª”åŒ¯å…¥æ™‚å¯åœ¨æ¯æª”ä¹‹é–“å‘¼å«)"""
        self._init_stats()

    def close(self):
        if self.conn:
            self.conn.close()
            self.conn = None


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ç¬¬äº”å±¤: å„ä¾†æºçš„ record è§£æå™¨
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _parse_csv_row(row: list) -> Optional[dict]:
    """
    å°‡ä¸€åˆ— LVR CSV â†’ æ¨™æº– record dictã€‚
    å›å‚³ None è¡¨ç¤ºè·³éã€‚
    """
    while len(row) < 33:
        row.append('')

    raw_address = row[2]
    parsed = parse_address(raw_address, row[0])

    # é è¨ˆç®— dedup key (é¿å… fast_insert_records é‡è¤‡æ­£è¦åŒ–)
    addr_norm = strip_city(norm_addr_simple(raw_address)) if raw_address else ''
    date_str = row[7]
    d = date_str.replace('/', '')[:7] if date_str else ''
    price = safe_int(row[21]) or 0
    _dedup_key = f"{d}|{addr_norm}|{price}" if addr_norm else None

    return {
        'raw_district':      row[0],
        'transaction_type':  row[1],
        'address':           row[2],
        'land_area':         safe_float(row[3]),
        'urban_zone':        row[4],
        'non_urban_zone':    row[5],
        'non_urban_use':     row[6],
        'transaction_date':  row[7],
        'transaction_count': row[8],
        'floor_level':       row[9],
        'total_floors':      row[10],
        'building_type':     row[11],
        'main_use':          row[12],
        'main_material':     row[13],
        'build_date':        row[14],
        'building_area':     safe_float(row[15]),
        'rooms':             safe_int(row[16]),
        'halls':             safe_int(row[17]),
        'bathrooms':         safe_int(row[18]),
        'partitioned':       row[19],
        'has_management':    row[20],
        'total_price':       safe_int(row[21]),
        'unit_price':        safe_float(row[22]),
        'parking_type':      row[23],
        'parking_area':      safe_float(row[24]),
        'parking_price':     safe_int(row[25]),
        'note':              row[26],
        'serial_no':         row[27],
        'main_area':         safe_float(row[28]),
        'attached_area':     safe_float(row[29]),
        'balcony_area':      safe_float(row[30]),
        'elevator':          row[31],
        'transfer_no':       row[32] if len(row) > 32 else '',
        'county_city':       parsed['county_city'],
        'district':          parsed['district'],
        'village':           parsed['village'],
        'street':            parsed['street'],
        'lane':              parsed['lane'],
        'alley':             parsed['alley'],
        'number':            parsed['number'],
        'floor':             parsed['floor'],
        'sub_number':        parsed['sub_number'],
        'community_name':    None,
        'lat':               None,
        'lng':               None,
        '_dedup_key':        _dedup_key,
    }


def _parse_csv_row_fast(row: list):
    """
    å°‡ä¸€åˆ— LVR CSV â†’ (values_tuple, dedup_key) å¿«é€Ÿç‰ˆã€‚
    ç›´æ¥ç”¢ç”Ÿ INSERT ç”¨çš„ tupleï¼Œé¿å… dict å‰µå»º + å†æå–çš„é–‹éŠ·ã€‚
    å›å‚³ None è¡¨ç¤ºè·³éã€‚
    """
    while len(row) < 33:
        row.append('')

    raw_address = row[2]
    parsed = parse_address(raw_address, row[0])

    # é è¨ˆç®— dedup key
    addr_norm = strip_city(norm_addr_simple(raw_address)) if raw_address else ''
    d = row[7].replace('/', '')[:7] if row[7] else ''
    price = safe_int(row[21]) or 0
    dedup_key = f"{d}|{addr_norm}|{price}" if addr_norm else None

    # ç›´æ¥å»ºç«‹èˆ‡ LAND_COLUMNS + ['dedup_key'] å°æ‡‰çš„ tuple
    return (
        row[0],                          # raw_district
        row[1],                          # transaction_type
        row[2],                          # address
        safe_float(row[3]),              # land_area
        row[4],                          # urban_zone
        row[5],                          # non_urban_zone
        row[6],                          # non_urban_use
        row[7],                          # transaction_date
        row[8],                          # transaction_count
        row[9],                          # floor_level
        row[10],                         # total_floors
        row[11],                         # building_type
        row[12],                         # main_use
        row[13],                         # main_material
        row[14],                         # build_date
        safe_float(row[15]),             # building_area
        safe_int(row[16]),               # rooms
        safe_int(row[17]),               # halls
        safe_int(row[18]),               # bathrooms
        row[19],                         # partitioned
        row[20],                         # has_management
        safe_int(row[21]),               # total_price
        safe_float(row[22]),             # unit_price
        row[23],                         # parking_type
        safe_float(row[24]),             # parking_area
        safe_int(row[25]),               # parking_price
        row[26],                         # note
        row[27],                         # serial_no
        safe_float(row[28]),             # main_area
        safe_float(row[29]),             # attached_area
        safe_float(row[30]),             # balcony_area
        row[31],                         # elevator
        row[32] if len(row) > 32 else '',  # transfer_no
        parsed['county_city'],           # county_city
        parsed['district'],              # district
        parsed['village'],               # village
        parsed['street'],                # street
        parsed['lane'],                  # lane
        parsed['alley'],                 # alley
        parsed['number'],                # number
        parsed['floor'],                 # floor
        parsed['sub_number'],            # sub_number
        None,                            # community_name
        None,                            # lat
        None,                            # lng
        dedup_key,                       # dedup_key
    )


def _parse_api_row(row) -> Optional[dict]:
    """
    å°‡ transactions.db ä¸€åˆ— â†’ æ¨™æº– record dictã€‚
    å›å‚³ None è¡¨ç¤ºè³‡æ–™ç¼ºæã€‚
    """
    _id, city_code, town, addr_raw, build_type, community, \
        date_str, floor_col, area_col, tp_raw, up_raw, \
        lat, lon, sq, rj_text = row

    j = {}
    if rj_text:
        try:
            j = json.loads(rj_text)
        except Exception:
            pass

    # åœ°å€æ¸…æ´—
    addr_clean = clean_trans_addr(addr_raw)
    if not addr_clean or 'è™Ÿ' not in addr_clean:
        return None

    # ç”¨ city_code å–å¾— city_hint â†’ ç²¾ç¢ºæ¶ˆæ­§
    city_hint = CITY_CODE_MAP.get(city_code, '')
    parsed = parse_address(addr_clean, city_hint=city_hint)

    # æ—¥æœŸ
    transaction_date = normalize_date(date_str)

    # æ¨“å±¤
    floor_json = j.get('f', '') or floor_col or ''
    floor_level, total_floors = parse_floor_info(floor_json)

    # JSON æ¬„ä½
    transaction_type = j.get('t', '') or ''
    rooms = safe_int(j.get('j'))
    halls = safe_int(j.get('k'))
    bathrooms = safe_int(j.get('l'))
    has_management = j.get('m', '') or ''
    main_use = j.get('pu', '') or j.get('AA11', '') or ''
    building_type_j = build_type or j.get('b', '') or ''
    note = j.get('note', '') or ''

    total_price = parse_price(tp_raw) or parse_price(j.get('tp'))
    unit_price = safe_float(up_raw) or safe_float(j.get('cp'))
    building_area = safe_float(area_col) or safe_float(j.get('s'))

    serial_no = f'api_{sq}' if sq else None

    lat_val = lat if (lat and lat != 0) else None
    lng_val = lon if (lon and lon != 0) else None
    if not lat_val and j.get('lat'):
        lat_val = j['lat']
    if not lng_val and j.get('lon'):
        lng_val = j['lon']

    floor_parsed = parsed['floor']
    if not floor_parsed and floor_level:
        try:
            int(floor_level)
            floor_parsed = floor_level
        except ValueError:
            pass

    return {
        'raw_district':      parsed.get('district') or town or '',
        'transaction_type':  transaction_type,
        'address':           addr_clean,
        'land_area':         None,
        'urban_zone':        '',
        'non_urban_zone':    '',
        'non_urban_use':     '',
        'transaction_date':  transaction_date,
        'transaction_count': '',
        'floor_level':       floor_level,
        'total_floors':      total_floors,
        'building_type':     building_type_j,
        'main_use':          main_use,
        'main_material':     '',
        'build_date':        '',
        'building_area':     building_area,
        'rooms':             rooms,
        'halls':             halls,
        'bathrooms':         bathrooms,
        'partitioned':       '',
        'has_management':    has_management,
        'total_price':       total_price,
        'unit_price':        unit_price,
        'parking_type':      '',
        'parking_area':      None,
        'parking_price':     None,
        'note':              note,
        'serial_no':         serial_no,
        'main_area':         None,
        'attached_area':     None,
        'balcony_area':      None,
        'elevator':          '',
        'transfer_no':       '',
        'county_city':       parsed['county_city'],
        'district':          parsed['district'],
        'village':           parsed['village'],
        'street':            parsed['street'],
        'lane':              parsed['lane'],
        'alley':             parsed['alley'],
        'number':            parsed['number'],
        'floor':             floor_parsed,
        'sub_number':        parsed['sub_number'],
        'community_name':    community or '',
        'lat':               lat_val,
        'lng':               lng_val,
    }


# èˆŠç‰ˆç›¸å®¹: tuple æ ¼å¼çš„ API è§£æ (ä¾› load_api ä½¿ç”¨)
def _parse_api_record(row):
    """[å‘å¾Œç›¸å®¹] å›å‚³ tuple æ ¼å¼"""
    rec = _parse_api_row(row)
    if rec is None:
        return None
    return tuple(rec.get(col) for col in LAND_COLUMNS)


def _parse_land_db_row(row, col_names: list) -> Optional[dict]:
    """
    å°‡å¦ä¸€å€‹ land_data.db çš„ä¸€åˆ— â†’ æ¨™æº– record dictã€‚
    (ç”¨æ–¼åˆä½µå…©å€‹ land_data.db)
    """
    rec = {}
    for i, col in enumerate(col_names):
        if col == 'id':
            continue
        rec[col] = row[i]
    return rec


def _parse_generic_csv_row(row: list, header_map: dict) -> Optional[dict]:
    """
    å°‡é€šç”¨ CSV ä¸€åˆ— â†’ æ¨™æº– record dictã€‚
    header_map: {csvæ¬„ä½å â†’ land_dataæ¬„ä½å}
    """
    rec = {col: None for col in LAND_COLUMNS}

    for csv_col, land_col in header_map.items():
        if csv_col == '_indices':
            continue
        idx = header_map['_indices'].get(csv_col)
        if idx is not None and idx < len(row):
            val = row[idx]
            # ä¾æ“šæ¬„ä½é¡å‹è½‰æ›
            if land_col in ('land_area', 'building_area', 'unit_price',
                            'parking_area', 'main_area', 'attached_area',
                            'balcony_area'):
                rec[land_col] = safe_float(val)
            elif land_col in ('rooms', 'halls', 'bathrooms', 'total_price',
                              'parking_price'):
                rec[land_col] = safe_int(val)
            else:
                rec[land_col] = val or ''

    # å¦‚æœæœ‰åœ°å€ï¼Œåšçµæ§‹åŒ–è§£æ
    addr = rec.get('address', '')
    if addr:
        district_hint = rec.get('raw_district', '') or rec.get('district', '') or ''
        parsed = parse_address(addr, district_hint)
        for k in ('county_city', 'district', 'village', 'street',
                   'lane', 'alley', 'number', 'floor', 'sub_number'):
            if not rec.get(k):
                rec[k] = parsed.get(k, '')

    return rec


# é€šç”¨ CSV æ¬„ä½åç¨±æ˜ å°„ (csv header â†’ land_data column)
_GENERIC_CSV_MAP = {
    # ä¸­æ–‡
    'é„‰é®å¸‚å€': 'raw_district', 'äº¤æ˜“æ¨™çš„': 'transaction_type',
    'åœŸåœ°ä½ç½®å»ºç‰©é–€ç‰Œ': 'address', 'åœ°å€': 'address', 'é–€ç‰Œ': 'address',
    'åœŸåœ°ç§»è½‰ç¸½é¢ç©å¹³æ–¹å…¬å°º': 'land_area', 'åœŸåœ°é¢ç©': 'land_area',
    'éƒ½å¸‚åœŸåœ°ä½¿ç”¨åˆ†å€': 'urban_zone', 'ééƒ½å¸‚åœŸåœ°ä½¿ç”¨åˆ†å€': 'non_urban_zone',
    'ééƒ½å¸‚åœŸåœ°ä½¿ç”¨ç·¨å®š': 'non_urban_use',
    'äº¤æ˜“å¹´æœˆæ—¥': 'transaction_date', 'äº¤æ˜“æ—¥æœŸ': 'transaction_date',
    'äº¤æ˜“ç­†æ£Ÿæ•¸': 'transaction_count',
    'ç§»è½‰å±¤æ¬¡': 'floor_level', 'ç¸½æ¨“å±¤æ•¸': 'total_floors',
    'å»ºç‰©å‹æ…‹': 'building_type', 'ä¸»è¦ç”¨é€”': 'main_use',
    'ä¸»è¦å»ºæ': 'main_material', 'å»ºç¯‰å®Œæˆå¹´æœˆ': 'build_date',
    'å»ºç‰©ç§»è½‰ç¸½é¢ç©å¹³æ–¹å…¬å°º': 'building_area', 'å»ºç‰©é¢ç©': 'building_area',
    'å»ºç‰©ç¾æ³æ ¼å±€-æˆ¿': 'rooms', 'æˆ¿': 'rooms',
    'å»ºç‰©ç¾æ³æ ¼å±€-å»³': 'halls', 'å»³': 'halls',
    'å»ºç‰©ç¾æ³æ ¼å±€-è¡›': 'bathrooms', 'è¡›': 'bathrooms',
    'å»ºç‰©ç¾æ³æ ¼å±€-éš”é–“': 'partitioned', 'æœ‰ç„¡ç®¡ç†çµ„ç¹”': 'has_management',
    'ç¸½åƒ¹å…ƒ': 'total_price', 'ç¸½åƒ¹': 'total_price',
    'å–®åƒ¹å…ƒå¹³æ–¹å…¬å°º': 'unit_price', 'å–®åƒ¹': 'unit_price',
    'è»Šä½é¡åˆ¥': 'parking_type',
    'è»Šä½ç§»è½‰ç¸½é¢ç©(å¹³æ–¹å…¬å°º)': 'parking_area', 'è»Šä½é¢ç©': 'parking_area',
    'è»Šä½ç¸½åƒ¹å…ƒ': 'parking_price', 'è»Šä½ç¸½åƒ¹': 'parking_price',
    'å‚™è¨»': 'note', 'ç·¨è™Ÿ': 'serial_no',
    'ä¸»å»ºç‰©é¢ç©': 'main_area', 'é™„å±¬å»ºç‰©é¢ç©': 'attached_area',
    'é™½å°é¢ç©': 'balcony_area', 'é›»æ¢¯': 'elevator', 'ç§»è½‰ç·¨è™Ÿ': 'transfer_no',
    'ç¸£å¸‚': 'county_city', 'å€': 'district', 'ç¤¾å€': 'community_name',
    'ç·¯åº¦': 'lat', 'ç¶“åº¦': 'lng',
    # è‹±æ–‡
    'address': 'address', 'total_price': 'total_price',
    'unit_price': 'unit_price', 'transaction_date': 'transaction_date',
    'district': 'district', 'county_city': 'county_city',
    'community': 'community_name', 'lat': 'lat', 'lng': 'lng', 'lon': 'lng',
}


def _build_generic_csv_map(headers: list) -> dict:
    """å¾ CSV header å»ºç«‹æ¬„ä½æ˜ å°„"""
    mapping = {}
    indices = {}
    for i, h in enumerate(headers):
        h_clean = h.strip()
        if h_clean in _GENERIC_CSV_MAP:
            land_col = _GENERIC_CSV_MAP[h_clean]
            mapping[h_clean] = land_col
            indices[h_clean] = i
    mapping['_indices'] = indices
    return mapping


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ç¬¬å…­å±¤: åŒ¯å…¥å¼•æ“ (è®€å–å„ä¾†æº â†’ å‘¼å« db.upsert_record)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def import_csv_lvr(db: LandDataDB, csv_path: str):
    """åŒ¯å…¥ LVR å¯¦åƒ¹ç™»éŒ„ CSV (ä½¿ç”¨æ¥µé€Ÿ tuple æ’å…¥)"""
    log_print(f'\nğŸ“„ [CSV-LVR] åŒ¯å…¥: {csv_path}')
    t0 = time.time()

    batch = []
    batch_size = db.BATCH_SIZE
    total = 0

    with open(csv_path, 'r', encoding='utf-8-sig') as f:
        reader = csv.reader(f)
        next(reader, None)  # ä¸­æ–‡æ¨™é ­
        next(reader, None)  # è‹±æ–‡æ¨™é ­

        for row in reader:
            tup = _parse_csv_row_fast(row)
            if tup:
                batch.append(tup)

            total += 1
            if len(batch) >= batch_size:
                db.fast_insert_tuples(batch)
                db.conn.commit()
                batch = []

                elapsed = time.time() - t0
                rate = total / elapsed if elapsed > 0 else 0
                s = db._stats
                log_print(f'  â³ {total:,} ç­† | æ–°å¢ {s["inserted"]:,} | '
                      f'è£œå…… {s["enriched"]:,} | é‡è¤‡ {s["duplicated"]:,} | '
                      f'ä¸Ÿæ£„ {s["discarded"]:,} ({rate:,.0f}/s)',
                      flush=True)

    if batch:
        db.fast_insert_tuples(batch)
        db.conn.commit()

    elapsed = time.time() - t0
    log_print(f'  âœ… CSV-LVR å®Œæˆ: {elapsed:.1f}s')


def import_csv_generic(db: LandDataDB, csv_path: str):
    """åŒ¯å…¥é€šç”¨ CSV"""
    log_print(f'\nğŸ“„ [CSV-Generic] åŒ¯å…¥: {csv_path}')
    t0 = time.time()

    with open(csv_path, 'r', encoding='utf-8-sig') as f:
        reader = csv.reader(f)
        headers = next(reader, [])
        header_map = _build_generic_csv_map(headers)

        if not header_map.get('_indices'):
            log_print(f'  âš ï¸  ç„¡æ³•è­˜åˆ¥æ¬„ä½æ˜ å°„ï¼Œè·³éæ­¤æª”æ¡ˆ')
            log_print(f'     åµæ¸¬åˆ°çš„æ¬„ä½: {headers[:10]}...')
            return

        mapped = {k: v for k, v in header_map.items() if k != '_indices'}
        log_print(f'  æ¬„ä½æ˜ å°„: {mapped}')

        for i, row in enumerate(reader, 1):
            rec = _parse_generic_csv_row(row, header_map)
            if rec:
                db.upsert_record(rec)

            if i % 10000 == 0:
                db.flush_all()
                elapsed = time.time() - t0
                s = db._stats
                log_print(f'  â³ {i:,} ç­† | æ–°å¢ {s["inserted"]:,} | '
                      f'è£œå…… {s["enriched"]:,} | é‡è¤‡ {s["duplicated"]:,} | '
                      f'ä¸Ÿæ£„ {s["discarded"]:,} ({elapsed:.0f}s)',
                      flush=True)

    db.flush_all()
    elapsed = time.time() - t0
    log_print(f'  âœ… CSV-Generic å®Œæˆ: {elapsed:.1f}s')


def import_api_db(db: LandDataDB, api_db_path: str):
    """åŒ¯å…¥ API transactions DB (ä½¿ç”¨æ‰¹æ¬¡å¿«é€Ÿæ’å…¥)"""
    log_print(f'\nğŸŒ [API-DB] åŒ¯å…¥: {api_db_path}')
    t0 = time.time()

    conn_t = sqlite3.connect(api_db_path)
    conn_t.text_factory = lambda b: b.decode('utf-8', errors='replace')
    ct = conn_t.cursor()
    ct.execute(
        'SELECT id, city, town, address, build_type, community, date_str, '
        'floor, area, total_price, unit_price, lat, lon, sq, raw_json '
        'FROM transactions'
    )

    batch = []
    batch_size = db.BATCH_SIZE
    total = 0

    for row in ct:
        total += 1
        try:
            rec = _parse_api_row(row)
        except Exception:
            rec = None

        if rec:
            batch.append(rec)
        else:
            db._stats['discarded'] += 1
            db._stats['discard_parse_err'] += 1
            db._stats['total_scanned'] += 1

        if len(batch) >= batch_size:
            db.fast_insert_records(batch)
            db.conn.commit()
            batch = []

            elapsed = time.time() - t0
            rate = total / elapsed if elapsed > 0 else 0
            s = db._stats
            log_print(f'  â³ {total:,} ç­† | æ–°å¢ {s["inserted"]:,} | '
                  f'è£œå…… {s["enriched"]:,} | é‡è¤‡ {s["duplicated"]:,} | '
                  f'ä¸Ÿæ£„ {s["discarded"]:,} ({rate:,.0f}/s)',
                  flush=True)

    if batch:
        db.fast_insert_records(batch)
        db.conn.commit()

    conn_t.close()
    elapsed = time.time() - t0
    log_print(f'  âœ… API-DB å®Œæˆ: {elapsed:.1f}s')


def import_land_db(db: LandDataDB, source_db_path: str):
    """å¾å¦ä¸€å€‹ land_data.db åŒ¯å…¥ (åˆä½µå…©å€‹ land_data.db)"""
    print(f'\nğŸ“¦ [LAND-DB] åŒ¯å…¥: {source_db_path}')
    t0 = time.time()

    conn_s = sqlite3.connect(source_db_path)
    cur_s = conn_s.cursor()

    # å–å¾—ä¾†æºçš„æ¬„ä½å
    cur_s.execute('PRAGMA table_info(land_transaction)')
    col_names = [row[1] for row in cur_s.fetchall()]

    cur_s.execute('SELECT * FROM land_transaction')

    for i, row in enumerate(cur_s, 1):
        try:
            rec = _parse_land_db_row(row, col_names)
        except Exception:
            rec = None

        if rec:
            db.upsert_record(rec)
        else:
            db._stats['discarded'] += 1
            db._stats['total_scanned'] += 1

        if i % 10000 == 0:
            db.flush_all()
            elapsed = time.time() - t0
            s = db._stats
            print(f'  â³ {i:,} ç­† | æ–°å¢ {s["inserted"]:,} | '
                  f'è£œå…… {s["enriched"]:,} | é‡è¤‡ {s["duplicated"]:,} | '
                  f'ä¸Ÿæ£„ {s["discarded"]:,} ({elapsed:.0f}s)',
                  flush=True)

    db.flush_all()
    conn_s.close()
    elapsed = time.time() - t0
    print(f'  âœ… LAND-DB å®Œæˆ: {elapsed:.1f}s')


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ç¬¬ä¸ƒå±¤: ä¸»æµç¨‹
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def import_file(db: LandDataDB, filepath: str):
    """
    è‡ªå‹•åµæ¸¬ä¸¦åŒ¯å…¥å–®ä¸€æª”æ¡ˆã€‚
    """
    if not os.path.exists(filepath):
        log_print(f'  âŒ æª”æ¡ˆä¸å­˜åœ¨: {filepath}')
        return

    source_type = detect_source(filepath)
    log_print(f'  ğŸ” åµæ¸¬åˆ°ä¾†æºé¡å‹: {source_type.value}')

    if source_type == SourceType.CSV_LVR:
        import_csv_lvr(db, filepath)
    elif source_type == SourceType.CSV_GENERIC:
        import_csv_generic(db, filepath)
    elif source_type == SourceType.API_DB:
        import_api_db(db, filepath)
    elif source_type == SourceType.LAND_DB:
        # é˜²æ­¢è‡ªå·±åŒ¯å…¥è‡ªå·±
        target_real = os.path.realpath(db.db_path)
        source_real = os.path.realpath(filepath)
        if target_real == source_real:
            log_print(f'  âš ï¸  ä¾†æºèˆ‡ç›®æ¨™æ˜¯åŒä¸€å€‹æª”æ¡ˆï¼Œè·³é')
            return
        import_land_db(db, filepath)
    else:
        log_print(f'  âŒ ç„¡æ³•è­˜åˆ¥çš„è³‡æ–™ä¾†æºæ ¼å¼: {filepath}')
        return


def convert_v4(input_files: List[str], target_path: str,
               rebuild: bool = False, skip_finalize: bool = False,
               verbose: bool = False):
    """
    ä¸»è¦è½‰æ›æµç¨‹ (v4)ã€‚

    Args:
        input_files:    è¦åŒ¯å…¥çš„æª”æ¡ˆè·¯å¾‘åˆ—è¡¨
        target_path:    ç›®æ¨™ land_data.db è·¯å¾‘
        rebuild:        æ˜¯å¦é‡å»º (åˆªé™¤èˆŠ DB)
        skip_finalize:  è·³éç´¢å¼•/FTS/VACUUM (å¤šæ‰¹åŒ¯å…¥æ™‚æœ€å¾Œå†åš)
    """
    global _VERBOSE
    _VERBOSE = verbose

    log_path = os.path.join(os.path.dirname(target_path), 'land_data_import.log')
    init_logging(log_path)

    log_print(f'\n{"=" * 60}')
    log_print(f'  ç›®æ¨™:  {target_path}')
    log_print(f'  æ¨¡å¼:  {"é‡å»º" if rebuild else "å¢é‡åŒ¯å…¥"}')
    log_print(f'  è¼¸å…¥:  {len(input_files)} å€‹æª”æ¡ˆ')
    for f in input_files:
        log_print(f'         â€¢ {f}')
    log_print(f'  Verbose æ¨¡å¼: {verbose} (å…¨åŸŸ _VERBOSE={_VERBOSE})')
    if _VERBOSE:
        log_print(f'  è©³ç´°log: é–‹å•Ÿ (æ¯ç¨®é¡å‹å‰ {_VERBOSE_MAX} ç­†ç¯„ä¾‹)')
    log_print(f'{"=" * 60}')

    db = LandDataDB(target_path)
    db.open(rebuild=rebuild)

    t0 = time.time()

    # è¿½è¹¤æ˜¯å¦æœ‰ API DB (ç”¨æ–¼å¾ŒçºŒç¤¾å€å›å¡«)
    api_db_files = []

    for filepath in input_files:
        db.reset_stats()
        import_file(db, filepath)
        # ç¢ºä¿ flush all samples before printing stats
        db.flush_all()
        db.print_stats()

        # è¨˜ä¸‹ API DB è·¯å¾‘ä¾›ç¤¾å€å›å¡«
        st = detect_source(filepath)
        if st == SourceType.API_DB:
            api_db_files.append(filepath)

    # ç¤¾å€å›å¡« (è‹¥æœ‰ API DB ä¾†æº)
    for api_path in api_db_files:
        t_bf = time.time()
        bf_count = db.backfill_community(api_path)
        log_print(f'  âœ… ç¤¾å€å›å¡«: {bf_count:,} ç­† ({time.time() - t_bf:.1f}s)')

    # ç´¢å¼•/FTS/å£“ç¸®
    if not skip_finalize:
        db.finalize()

    elapsed = time.time() - t0
    log_print(f'\nğŸ‰ å…¨éƒ¨å®Œæˆ! è€—æ™‚ {elapsed:.1f}s')

    # æœ€çµ‚ç¸½è¦½
    db.reset_stats()
    db.print_stats()
    db.close()
    
    close_logging()
    log_print(f'ğŸ“ æ—¥èªŒå·²ä¿å­˜: {log_path}')


# â”€â”€ å‘å¾Œç›¸å®¹: èˆŠç‰ˆ v3 API â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def load_csv(conn, csv_path):
    """[å‘å¾Œç›¸å®¹] èˆŠç‰ˆ CSV è¼‰å…¥ (ç›´æ¥ INSERTï¼Œä¸åšå»é‡)"""
    print(f'\nğŸ“„ [CSV] è¼‰å…¥: {csv_path}')
    cursor = conn.cursor()
    batch, total, parsed_ok = [], 0, 0
    t0 = time.time()

    with open(csv_path, 'r', encoding='utf-8-sig') as f:
        reader = csv.reader(f)
        next(reader, None)
        next(reader, None)
        for row in reader:
            total += 1
            while len(row) < 33:
                row.append('')
            parsed = parse_address(row[2], row[0])
            if parsed['street']:
                parsed_ok += 1
            values = (
                row[0], row[1], row[2], safe_float(row[3]),
                row[4], row[5], row[6], row[7], row[8], row[9], row[10],
                row[11], row[12], row[13], row[14],
                safe_float(row[15]), safe_int(row[16]), safe_int(row[17]),
                safe_int(row[18]), row[19], row[20],
                safe_int(row[21]), safe_float(row[22]),
                row[23], safe_float(row[24]), safe_int(row[25]),
                row[26], row[27], safe_float(row[28]),
                safe_float(row[29]), safe_float(row[30]),
                row[31], row[32] if len(row) > 32 else '',
                parsed['county_city'], parsed['district'], parsed['village'],
                parsed['street'], parsed['lane'], parsed['alley'],
                parsed['number'], parsed['floor'], parsed['sub_number'],
                None, None, None,
            )
            batch.append(values)
            if len(batch) >= 10000:
                cursor.executemany(INSERT_SQL, batch)
                conn.commit()
                batch = []

    if batch:
        cursor.executemany(INSERT_SQL, batch)
        conn.commit()

    elapsed = time.time() - t0
    pct = parsed_ok / total * 100 if total else 0
    print(f'\n  âœ… CSV è¼‰å…¥å®Œæˆ: {total:,} ç­†, '
          f'åœ°å€è§£æç‡ {pct:.1f}%, {elapsed:.1f}s')
    return total


def load_api(conn, api_db_path):
    """[å‘å¾Œç›¸å®¹] èˆŠç‰ˆ API è¼‰å…¥ (ç›´æ¥ INSERTï¼Œä¸åšå»é‡)"""
    print(f'\nğŸŒ [API] è¼‰å…¥: {api_db_path}')
    cursor = conn.cursor()
    conn_t = sqlite3.connect(api_db_path)
    conn_t.text_factory = lambda b: b.decode('utf-8', errors='replace')
    ct = conn_t.cursor()
    ct.execute(
        'SELECT id, city, town, address, build_type, community, date_str, '
        'floor, area, total_price, unit_price, lat, lon, sq, raw_json '
        'FROM transactions'
    )
    batch, total, inserted, skipped = [], 0, 0, 0
    t0 = time.time()
    for row in ct:
        total += 1
        try:
            rec = _parse_api_record(row)
        except Exception:
            skipped += 1
            continue
        if rec is None:
            skipped += 1
            continue
        batch.append(rec)
        inserted += 1
        if len(batch) >= 10000:
            cursor.executemany(INSERT_SQL, batch)
            conn.commit()
            batch = []
    if batch:
        cursor.executemany(INSERT_SQL, batch)
        conn.commit()
    conn_t.close()
    elapsed = time.time() - t0
    print(f'\n  âœ… API è¼‰å…¥å®Œæˆ: æƒæ {total:,}, '
          f'æ’å…¥ {inserted:,}, ç•¥é {skipped:,}, {elapsed:.1f}s')
    return inserted


def create_tables(cursor):
    """[å‘å¾Œç›¸å®¹] å»ºç«‹è³‡æ–™è¡¨"""
    db = LandDataDB.__new__(LandDataDB)
    db._create_tables(cursor)


def create_indexes(cursor):
    """[å‘å¾Œç›¸å®¹] å»ºç«‹ç´¢å¼•"""
    print('  ğŸ“‡ å»ºç«‹ç´¢å¼•...')
    indexes = [
        ('idx_county_city', 'county_city'),
        ('idx_district', 'district'),
        ('idx_street', 'street'),
        ('idx_lane', 'lane'),
        ('idx_number', 'number'),
        ('idx_floor', 'floor'),
        ('idx_date', 'transaction_date'),
        ('idx_price', 'total_price'),
        ('idx_serial', 'serial_no'),
    ]
    for name, col in indexes:
        cursor.execute(f'CREATE INDEX IF NOT EXISTS {name} ON land_transaction({col})')
    cursor.execute('''CREATE INDEX IF NOT EXISTS idx_addr_combo
        ON land_transaction(county_city, district, street, lane, number)''')
    # com2address æŸ¥è©¢ç”¨ç´¢å¼•
    cursor.execute('CREATE INDEX IF NOT EXISTS idx_community_address ON land_transaction(community_name, address) WHERE community_name IS NOT NULL AND address IS NOT NULL')
    cursor.execute('CREATE INDEX IF NOT EXISTS idx_street_lane_district ON land_transaction(street, lane, district)')
    cursor.execute('CREATE INDEX IF NOT EXISTS idx_search_numbers ON land_transaction(street, lane, district, total_floors, build_date) WHERE number IS NOT NULL')


def create_fts(cursor):
    """[å‘å¾Œç›¸å®¹] å»ºç«‹ FTS5"""
    print('  ğŸ” å»ºç«‹ FTS5 å…¨æ–‡æª¢ç´¢...')
    cursor.execute('DROP TABLE IF EXISTS address_fts')
    cursor.execute('''
        CREATE VIRTUAL TABLE address_fts USING fts5(
            address, content='land_transaction', content_rowid='id',
            tokenize='unicode61'
        )
    ''')
    cursor.execute('''
        INSERT INTO address_fts(rowid, address)
        SELECT id, address FROM land_transaction WHERE address != ''
    ''')


def convert(source, csv_path=None, api_path=None, output_path=None):
    """[å‘å¾Œç›¸å®¹] v3 è½‰æ›æµç¨‹ â€” æœƒåˆªé™¤èˆŠ DB é‡å»º"""
    input_files = []
    if source in ('csv', 'both') and csv_path:
        input_files.append(csv_path)
    if source in ('api', 'both') and api_path:
        input_files.append(api_path)
    convert_v4(input_files, output_path, rebuild=True)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CLI
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def main():
    parser = argparse.ArgumentParser(
        description='å°ç£å¯¦åƒ¹ç™»éŒ„è³‡æ–™è½‰æ› v4 â€” è‡ªå‹•è­˜åˆ¥ + å¢é‡åŒ¯å…¥',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""ç¯„ä¾‹:
  # è‡ªå‹•åµæ¸¬ä¸¦å¢é‡åŒ¯å…¥ (æœ€å¸¸ç”¨)
  python3 convert.py data.csv
  python3 convert.py transactions.db
  python3 convert.py a.csv b.db c.csv

  # é‡å»º land_data.db (æ¸…ç©ºé‡ä¾†)
  python3 convert.py --rebuild data.csv transactions.db

  # æŒ‡å®šç›®æ¨™
  python3 convert.py --target /path/to/land_data.db data.csv

  # å‘å¾Œç›¸å®¹: ä¸å¸¶ input æ™‚èµ°é è¨­è·¯å¾‘ (csv + api â†’ both)
  python3 convert.py
  python3 convert.py --source csv
  python3 convert.py --source api
  python3 convert.py --source both
        """
    )
    parser.add_argument('inputs', nargs='*',
                        help='è¼¸å…¥æª”æ¡ˆè·¯å¾‘ (CSV / .db)ï¼Œå¯å¤šå€‹')
    parser.add_argument('--target', '-t', default=None,
                        help='ç›®æ¨™ land_data.db è·¯å¾‘')
    parser.add_argument('--rebuild', '-r', action='store_true',
                        help='é‡å»ºæ¨¡å¼: åˆªé™¤èˆŠ DB é‡æ–°åŒ¯å…¥')
    parser.add_argument('--skip-finalize', action='store_true',
                        help='è·³éå»ºç´¢å¼•/FTS/VACUUM (å¤šæ‰¹æ™‚æœ€å¾Œå†åš)')
    parser.add_argument('--verbose', '-v', action='store_true',
                        help='è©³ç´° log: é¡¯ç¤ºä¸Ÿæ£„/è£œå……/é‡è¤‡çš„ç¯„ä¾‹è¨˜éŒ„')

    # å‘å¾Œç›¸å®¹åƒæ•¸
    parser.add_argument('--source', '-s',
                        choices=['csv', 'api', 'both'], default=None,
                        help='[å‘å¾Œç›¸å®¹] è³‡æ–™ä¾†æºæ¨¡å¼')
    parser.add_argument('--csv-input', default=None,
                        help='[å‘å¾Œç›¸å®¹] CSV è¼¸å…¥è·¯å¾‘')
    parser.add_argument('--api-input', default=None,
                        help='[å‘å¾Œç›¸å®¹] API DB è·¯å¾‘')
    parser.add_argument('--output', '-o', default=None,
                        help='[å‘å¾Œç›¸å®¹] åŒ --target')

    args = parser.parse_args()

    script_dir = os.path.dirname(os.path.abspath(__file__))
    project_dir = os.path.dirname(script_dir)

    # ç›®æ¨™è·¯å¾‘
    target_path = (args.target or args.output
                   or os.path.join(project_dir, 'db', 'land_data.db'))

    # â€”â€” å‘å¾Œç›¸å®¹æ¨¡å¼: --source csv/api/both â€”â€”
    if args.source and not args.inputs:
        csv_path = args.csv_input or os.path.join(
            project_dir, 'db', 'ALL_lvr_land_a.csv')
        api_path = args.api_input or os.path.join(
            project_dir, 'db', 'transactions_all_original.db')

        input_files = []
        if args.source in ('csv', 'both'):
            if not os.path.exists(csv_path):
                print(f'âŒ æ‰¾ä¸åˆ° CSV æª”æ¡ˆ: {csv_path}')
                sys.exit(1)
            input_files.append(csv_path)
        if args.source in ('api', 'both'):
            if not os.path.exists(api_path):
                print(f'âŒ æ‰¾ä¸åˆ° API DB: {api_path}')
                sys.exit(1)
            input_files.append(api_path)

        # å‘å¾Œç›¸å®¹: --source æ¨¡å¼é è¨­ rebuild
        convert_v4(input_files, target_path, rebuild=True, verbose=args.verbose)
        return

    # â€”â€” æ–°ç‰ˆæ¨¡å¼: positional inputs â€”â€”
    if not args.inputs:
        # ç„¡è¼¸å…¥ â†’ é è¨­ both
        csv_path = os.path.join(project_dir, 'db', 'ALL_lvr_land_a.csv')
        api_path = os.path.join(project_dir, 'db', 'transactions_all_original.db')
        input_files = []
        if os.path.exists(csv_path):
            input_files.append(csv_path)
        if os.path.exists(api_path):
            input_files.append(api_path)
        if not input_files:
            print('âŒ æ‰¾ä¸åˆ°é è¨­è¼¸å…¥æª”æ¡ˆï¼Œè«‹æŒ‡å®šè¼¸å…¥è·¯å¾‘')
            parser.print_help()
            sys.exit(1)
        convert_v4(input_files, target_path, rebuild=True, verbose=args.verbose)
    else:
        # æœ‰æ˜ç¢º inputs â†’ å¢é‡åŒ¯å…¥ (é™¤é --rebuild)
        for f in args.inputs:
            if not os.path.exists(f):
                print(f'âŒ æª”æ¡ˆä¸å­˜åœ¨: {f}')
                sys.exit(1)
        convert_v4(args.inputs, target_path,
                   rebuild=args.rebuild,
                   skip_finalize=args.skip_finalize,
                   verbose=args.verbose)


if __name__ == '__main__':
    main()
