#!/usr/bin/env python3
"""
batch_geocode.py - æ‰¹æ¬¡åœ°ç†ç·¨ç¢¼ land_a.db æ‰€æœ‰åœ°å€
====================================================

å°‡ land_a.db ä¸­çš„äº¤æ˜“åœ°å€æ‰¹æ¬¡è½‰æ›ç‚ºç¶“ç·¯åº¦åº§æ¨™ï¼Œ
ä¸¦å¯é¸æ“‡å°‡çµæœå¯«å›è³‡æ–™åº«æˆ–åŒ¯å‡º CSVã€‚

ç”¨æ³•:
    # æŸ¥çœ‹ç›®å‰é€²åº¦
    python3 batch_geocode.py --status

    # è·¯æ®µç´šæ‰¹æ¬¡è™•ç†ï¼ˆæœ€å¿«ï¼Œæ¨è–¦ç¬¬ä¸€æ­¥ï¼‰
    python3 batch_geocode.py --strategy road

    # é™åˆ¶è™•ç†æ•¸é‡ï¼ˆæ¸¬è©¦ç”¨ï¼‰
    python3 batch_geocode.py --strategy road --limit 1000

    # è™•ç†ç‰¹å®šå€åŸŸ
    python3 batch_geocode.py --strategy road --district æ¾å±±å€

    # ç²¾ç¢ºåœ°å€ç´šè™•ç†ï¼ˆè¼ƒæ…¢ä½†ç²¾ç¢ºï¼‰
    python3 batch_geocode.py --strategy exact --limit 5000

    # å°‡çµæœå¯«å› land_a.dbï¼ˆæ–°å¢ lat/lng æ¬„ä½ï¼‰
    python3 batch_geocode.py --write-back

    # åŒ¯å‡ºå·² geocode çš„çµæœç‚º CSV
    python3 batch_geocode.py --export geocoded_addresses.csv

    # åŒ¯å…¥æ—¢æœ‰çš„ JSON å¿«å–
    python3 batch_geocode.py --import-cache ../../geocode_cache.json

    # ä½¿ç”¨æœ¬åœ° Nominatimï¼ˆé€Ÿåº¦é£›å‡ï¼‰
    python3 batch_geocode.py --strategy road --nominatim-url http://localhost:8080/search

ç’°å¢ƒéœ€æ±‚:
    pip install tqdm  (é¸ç”¨ï¼Œé¡¯ç¤ºé€²åº¦æ¢)
"""

import sqlite3
import os
import sys
import json
import csv
import time
import argparse
import logging
from pathlib import Path
from collections import defaultdict

# åŠ å…¥æ¨¡çµ„è·¯å¾‘
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, SCRIPT_DIR)

from geocoder import (
    TaiwanGeocoder, GeoCache, AddressNormalizer,
    DISTRICT_TO_CITY
)

logger = logging.getLogger(__name__)

# land_a.db è·¯å¾‘
DEFAULT_DB = os.path.join(SCRIPT_DIR, '..', '..', 'db', 'land_a.db')


class LandDBProcessor:
    """
    land_a.db æ‰¹æ¬¡åœ°ç†ç·¨ç¢¼è™•ç†å™¨

    å·¥ä½œæµç¨‹:
    1. å¾ land_a.db è®€å–ä¸åŒåœ°å€
    2. æ­£è¦åŒ– + å¿«å–æŸ¥è©¢
    3. API æŸ¥è©¢æœªå¿«å–çš„åœ°å€
    4. å„²å­˜çµæœåˆ°å¿«å–
    5. ï¼ˆå¯é¸ï¼‰å¯«å› land_a.db
    """

    def __init__(self, db_path: str = None, geocoder: TaiwanGeocoder = None):
        self.db_path = db_path or DEFAULT_DB
        if not os.path.exists(self.db_path):
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°è³‡æ–™åº«: {self.db_path}")

        self.geocoder = geocoder or TaiwanGeocoder()
        self.normalizer = AddressNormalizer()

    def get_status(self) -> dict:
        """å–å¾—ç›®å‰ç‹€æ…‹çµ±è¨ˆ"""
        con = sqlite3.connect(self.db_path)
        cur = con.cursor()

        stats = {}

        # ç¸½ç­†æ•¸
        stats['total_rows'] = cur.execute(
            "SELECT COUNT(*) FROM transactions"
        ).fetchone()[0]

        # æœ‰æ•ˆåœ°å€æ•¸
        stats['valid_addresses'] = cur.execute("""
            SELECT COUNT(DISTINCT address) FROM transactions
            WHERE address IS NOT NULL AND address != ''
              AND (address LIKE '%è·¯%' OR address LIKE '%è¡—%' OR address LIKE '%å¤§é“%')
              AND address LIKE '%è™Ÿ%'
              AND address NOT LIKE '%åœ°è™Ÿ%'
        """).fetchone()[0]

        # æª¢æŸ¥æ˜¯å¦å·²æœ‰ lat/lng æ¬„ä½
        cols = [row[1] for row in cur.execute("PRAGMA table_info(transactions)").fetchall()]
        stats['has_geocode_columns'] = 'lat' in cols and 'lng' in cols

        if stats['has_geocode_columns']:
            stats['geocoded_rows'] = cur.execute(
                "SELECT COUNT(*) FROM transactions WHERE lat IS NOT NULL AND lng IS NOT NULL"
            ).fetchone()[0]
        else:
            stats['geocoded_rows'] = 0

        # å€åŸŸåˆ†å¸ƒ
        stats['districts'] = dict(cur.execute("""
            SELECT district, COUNT(DISTINCT address)
            FROM transactions
            WHERE address IS NOT NULL AND address LIKE '%è™Ÿ%'
              AND address NOT LIKE '%åœ°è™Ÿ%'
            GROUP BY district
            ORDER BY COUNT(DISTINCT address) DESC
            LIMIT 20
        """).fetchall())

        # å¿«å–çµ±è¨ˆ
        stats['cache'] = self.geocoder.stats()

        con.close()
        return stats

    def get_unique_addresses(self, district: str = None,
                              limit: int = None) -> list:
        """
        å–å¾—ä¸åŒçš„æœ‰æ•ˆåœ°å€åˆ—è¡¨

        Returns: [(district, address), ...]
        """
        con = sqlite3.connect(self.db_path)
        cur = con.cursor()

        query = """
            SELECT DISTINCT district, address
            FROM transactions
            WHERE address IS NOT NULL AND address != ''
              AND (address LIKE '%è·¯%' OR address LIKE '%è¡—%' OR address LIKE '%å¤§é“%')
              AND address LIKE '%è™Ÿ%'
              AND address NOT LIKE '%åœ°è™Ÿ%'
        """
        params = []

        if district:
            query += " AND district = ?"
            params.append(district)

        query += " ORDER BY district, address"

        if limit:
            query += " LIMIT ?"
            params.append(limit)

        results = cur.execute(query, params).fetchall()
        con.close()

        return [(row[0], row[1]) for row in results]

    def get_unique_roads(self, district: str = None) -> list:
        """
        å–å¾—ä¸åŒè·¯æ®µåˆ—è¡¨

        Returns: [(district, road_name, address_count), ...]
        """
        addresses = self.get_unique_addresses(district)
        road_counts = defaultdict(lambda: {'count': 0, 'district': ''})

        for dist, addr in addresses:
            full = self.normalizer.build_full_address(addr, dist)
            if not full:
                continue
            road = self.normalizer.extract_road(full)
            if road:
                city_prefix = ''
                if dist in DISTRICT_TO_CITY:
                    city_prefix = DISTRICT_TO_CITY[dist] + dist
                elif dist:
                    city_prefix = dist
                road_key = f"{city_prefix}{road}"
                road_counts[road_key]['count'] += 1
                road_counts[road_key]['district'] = dist

        result = [
            (v['district'], road_key, v['count'])
            for road_key, v in road_counts.items()
        ]
        result.sort(key=lambda x: -x[2])
        return result

    def add_geocode_columns(self):
        """åœ¨ land_a.db æ–°å¢ lat/lng/geocode_level æ¬„ä½"""
        con = sqlite3.connect(self.db_path)
        cur = con.cursor()

        cols = [row[1] for row in cur.execute("PRAGMA table_info(transactions)").fetchall()]

        added = []
        if 'lat' not in cols:
            cur.execute("ALTER TABLE transactions ADD COLUMN lat REAL")
            added.append('lat')
        if 'lng' not in cols:
            cur.execute("ALTER TABLE transactions ADD COLUMN lng REAL")
            added.append('lng')
        if 'geocode_level' not in cols:
            cur.execute("ALTER TABLE transactions ADD COLUMN geocode_level TEXT")
            added.append('geocode_level')

        if added:
            # å»ºç«‹ç´¢å¼•åŠ é€ŸæŸ¥è©¢
            cur.execute("""
                CREATE INDEX IF NOT EXISTS idx_transactions_latlng
                ON transactions(lat, lng)
                WHERE lat IS NOT NULL
            """)
            con.commit()
            print(f"âœ… æ–°å¢æ¬„ä½: {', '.join(added)}")
        else:
            print(f"â„¹ï¸  æ¬„ä½å·²å­˜åœ¨ï¼Œç„¡éœ€æ–°å¢")

        con.close()

    def write_back(self, progress: bool = True):
        """
        å°‡å¿«å–çš„ geocode çµæœå¯«å› land_a.db

        ç­–ç•¥ï¼š
        1. ç”¨æ­£è¦åŒ–å¾Œçš„åŸºæœ¬åœ°å€æ¯”å°å¿«å–
        2. æ¯”å°ä¸åˆ°çš„ç”¨è·¯æ®µç´šåº§æ¨™
        """
        # ç¢ºä¿æ¬„ä½å­˜åœ¨
        self.add_geocode_columns()

        con = sqlite3.connect(self.db_path)
        con.execute("PRAGMA journal_mode=WAL")  # åŠ é€Ÿå¯«å…¥
        cur = con.cursor()

        # è®€å–æ‰€æœ‰éœ€è¦ geocode çš„ row
        rows = cur.execute("""
            SELECT id, district, address FROM transactions
            WHERE address IS NOT NULL AND address != ''
              AND (address LIKE '%è·¯%' OR address LIKE '%è¡—%' OR address LIKE '%å¤§é“%')
              AND address LIKE '%è™Ÿ%'
              AND address NOT LIKE '%åœ°è™Ÿ%'
              AND lat IS NULL
        """).fetchall()

        total = len(rows)
        if total == 0:
            print("â„¹ï¸  æ‰€æœ‰æœ‰æ•ˆåœ°å€å·²å®Œæˆ geocode")
            con.close()
            return

        if progress:
            print(f"\nğŸ“ å¯«å› land_a.db ({total:,} ç­†å¾…è™•ç†)")

        # å…ˆæ”¶é›†æ‰€æœ‰éœ€è¦æŸ¥è©¢çš„ key
        lookup_keys = set()
        row_to_keys = {}

        for row_id, district, address in rows:
            full_addr = self.normalizer.build_full_address(address, district)
            if not full_addr:
                continue
            base_addr = self.normalizer.extract_base_address(full_addr)
            if not base_addr:
                base_addr = full_addr

            # è·¯æ®µ key
            road = self.normalizer.extract_road(full_addr)
            road_key = None
            if road:
                city_prefix = ''
                if district in DISTRICT_TO_CITY:
                    city_prefix = DISTRICT_TO_CITY[district] + district
                elif district:
                    city_prefix = district
                road_key = f"{city_prefix}{road}"
                lookup_keys.add(road_key)

            lookup_keys.add(base_addr)
            row_to_keys[row_id] = (base_addr, road_key)

        # æ‰¹æ¬¡æŸ¥å¿«å–
        if progress:
            print(f"   æŸ¥è©¢å¿«å– ({len(lookup_keys):,} å€‹ key)...")

        all_cached = self.geocoder.cache.get_batch(list(lookup_keys))

        # æ‰¹æ¬¡æ›´æ–°
        updates = []
        matched = 0

        for row_id, (base_addr, road_key) in row_to_keys.items():
            lat = lng = level = None

            # ç²¾ç¢ºåŒ¹é…
            if base_addr in all_cached:
                c = all_cached[base_addr]
                lat, lng, level = c['lat'], c['lng'], c.get('level', 'exact')
            # è·¯æ®µåŒ¹é…
            elif road_key and road_key in all_cached:
                c = all_cached[road_key]
                lat, lng, level = c['lat'], c['lng'], 'road'

            if lat is not None:
                updates.append((lat, lng, level, row_id))
                matched += 1

        if updates:
            if progress:
                print(f"   æ›´æ–° {len(updates):,} ç­†...")

            # åˆ†æ‰¹æ›´æ–°é¿å…è¨˜æ†¶é«”çˆ†ç‚¸
            batch_size = 10000
            for i in range(0, len(updates), batch_size):
                batch = updates[i:i+batch_size]
                cur.executemany(
                    "UPDATE transactions SET lat=?, lng=?, geocode_level=? WHERE id=?",
                    batch
                )
                con.commit()
                if progress:
                    done = min(i + batch_size, len(updates))
                    print(f"   å·²å¯«å…¥: {done:,}/{len(updates):,}")

        if progress:
            print(f"\nâœ… å¯«å›å®Œæˆ: {matched:,}/{total:,} ç­†å·²æ›´æ–°")

        con.close()

    def upgrade_road_to_exact(self, progress: bool = True, dry_run: bool = False):
        """
        å°‡å·²å¯«å…¥çš„è·¯æ®µç´šåº§æ¨™ï¼ˆroadï¼‰å‡ç´šç‚ºç²¾ç¢ºé–€ç‰Œç´šï¼ˆexactï¼‰

        ä½¿ç”¨ OSM æœ¬åœ°ç´¢å¼•é‡æ–°æŸ¥è©¢ï¼Œåªæ›´æ–°èƒ½æ‰¾åˆ°ç²¾ç¢ºåº§æ¨™çš„è¨˜éŒ„ã€‚
        é©ç”¨æ–¼å»ºç«‹ OSM ç´¢å¼•å¾Œï¼Œé‡æ–°åˆ·æ–°èˆŠçš„è·¯æ®µç´šçµæœã€‚
        """
        if not self.geocoder.osm_index.is_available():
            print("âŒ OSM ç´¢å¼•å°šæœªå»ºç«‹ï¼Œè«‹å…ˆåŸ·è¡Œ build_osm_index.py")
            return

        self.add_geocode_columns()

        con = sqlite3.connect(self.db_path)
        con.execute("PRAGMA journal_mode=WAL")
        cur = con.cursor()

        # å–å¾—æ‰€æœ‰ã€Œè·¯æ®µç´šã€åº§æ¨™çš„è¨˜éŒ„
        rows = cur.execute("""
            SELECT id, district, address FROM transactions
            WHERE geocode_level = 'road'
              AND address IS NOT NULL
              AND address LIKE '%è™Ÿ%'
              AND address NOT LIKE '%åœ°è™Ÿ%'
        """).fetchall()

        total = len(rows)
        if total == 0:
            print("â„¹ï¸  æ²’æœ‰è·¯æ®µç´šè¨˜éŒ„éœ€è¦å‡ç´š")
            con.close()
            return

        if progress:
            print(f"\nğŸ”„ å‡ç´šè·¯æ®µç´šåº§æ¨™ â†’ ç²¾ç¢ºé–€ç‰Œ ({total:,} ç­†)")
            if dry_run:
                print("   [è©¦è·‘æ¨¡å¼ï¼Œä¸å¯¦éš›å¯«å…¥]")

        # æ”¶é›†æ‰€æœ‰åœ°å€åšæ‰¹æ¬¡æŸ¥è©¢
        base_addrs = {}
        for row_id, district, address in rows:
            full_addr = self.normalizer.build_full_address(address, district)
            if not full_addr:
                continue
            base_addr = self.normalizer.extract_base_address(full_addr)
            if not base_addr:
                base_addr = full_addr
            base_addrs[row_id] = base_addr

        # æ‰¹æ¬¡ OSM æŸ¥è©¢
        unique_addrs = list(set(base_addrs.values()))
        if progress:
            print(f"   ä¸åŒåœ°å€: {len(unique_addrs):,}")

        batch_size = 5000
        all_osm_results = {}
        for i in range(0, len(unique_addrs), batch_size):
            batch = unique_addrs[i:i+batch_size]
            results = self.geocoder.osm_index.batch_geocode(batch)
            all_osm_results.update(results)
            if progress:
                print(f"   æŸ¥è©¢é€²åº¦: {min(i+batch_size, len(unique_addrs)):,}/{len(unique_addrs):,} | å‘½ä¸­: {len(all_osm_results):,}")

        # çµ„åˆæ›´æ–°
        updates = []
        for row_id, base_addr in base_addrs.items():
            if base_addr in all_osm_results:
                r = all_osm_results[base_addr]
                updates.append((r['lat'], r['lng'], 'exact', row_id))

        if progress:
            hit_rate = len(updates) / max(len(rows), 1) * 100
            print(f"   ç²¾ç¢ºå‘½ä¸­: {len(updates):,}/{total:,} ({hit_rate:.1f}%)")

        if not dry_run and updates:
            for i in range(0, len(updates), 10000):
                batch = updates[i:i+10000]
                cur.executemany(
                    "UPDATE transactions SET lat=?, lng=?, geocode_level=? WHERE id=?",
                    batch
                )
                con.commit()
            print(f"âœ… å‡ç´šå®Œæˆï¼š{len(updates):,} ç­†å·²æ›´æ–°ç‚ºç²¾ç¢ºé–€ç‰Œåº§æ¨™")
        elif dry_run:
            print(f"[è©¦è·‘] é è¨ˆå‡ç´šï¼š{len(updates):,}/{total:,} ç­†")

        con.close()

    def export_csv(self, output_path: str, limit: int = None):
        """åŒ¯å‡º geocode çµæœç‚º CSV"""
        con = sqlite3.connect(self.db_path)
        cur = con.cursor()

        # å¾å¿«å–åŒ¯å‡º
        cache_db = self.geocoder.cache.db_path
        cache_con = sqlite3.connect(cache_db)

        query = """
            SELECT address_key, lat, lng, level, source, created_at
            FROM geocode_cache
            ORDER BY created_at DESC
        """
        if limit:
            query += f" LIMIT {limit}"

        rows = cache_con.execute(query).fetchall()
        cache_con.close()
        con.close()

        with open(output_path, 'w', newline='', encoding='utf-8-sig') as f:
            writer = csv.writer(f)
            writer.writerow(['address', 'lat', 'lng', 'level', 'source', 'created_at'])
            writer.writerows(rows)

        print(f"âœ… åŒ¯å‡º {len(rows):,} ç­†åˆ° {output_path}")


def print_status(processor: LandDBProcessor):
    """å°å‡ºç‹€æ…‹å ±å‘Š"""
    stats = processor.get_status()

    print("=" * 60)
    print("ğŸ“Š land_a.db åœ°ç†ç·¨ç¢¼ç‹€æ…‹")
    print("=" * 60)

    print(f"\nğŸ“‹ è³‡æ–™åº«:")
    print(f"   ç¸½äº¤æ˜“ç­†æ•¸:       {stats['total_rows']:>12,}")
    print(f"   æœ‰æ•ˆé–€ç‰Œåœ°å€æ•¸:    {stats['valid_addresses']:>12,}")

    if stats['has_geocode_columns']:
        pct = stats['geocoded_rows'] / max(stats['valid_addresses'], 1) * 100
        print(f"   å·² geocode ç­†æ•¸:  {stats['geocoded_rows']:>12,} ({pct:.1f}%)")
    else:
        print(f"   lat/lng æ¬„ä½:     âŒ å°šæœªå»ºç«‹")

    cache = stats['cache']
    print(f"\nğŸ’¾ å¿«å–:")
    print(f"   å¿«å–ç¸½æ•¸:          {cache['total']:>12,}")
    if cache.get('by_level'):
        for level, count in cache['by_level'].items():
            print(f"     {level:15s}  {count:>10,}")
    if cache.get('by_source'):
        print(f"   ä¾†æºåˆ†å¸ƒ:")
        for source, count in cache['by_source'].items():
            print(f"     {source:15s}  {count:>10,}")

    # OSM ç´¢å¼•ç‹€æ…‹
    from geocoder import OSMIndexProvider
    osm = processor.geocoder.osm_index if hasattr(processor.geocoder, 'osm_index') else OSMIndexProvider()
    print(f"\nğŸ  OSM é–€ç‰Œç´¢å¼•ï¼ˆç²¾ç¢ºæŸ¥è©¢ï¼‰:")
    if osm.is_available():
        print(f"   ç‹€æ…‹:              âœ… å¯ç”¨ ({osm.node_count:,} å€‹ç¯€é»)")
        print(f"   ç²¾åº¦:              é–€ç‰Œç´š (Â±10-50m)")
        print(f"   åŸ·è¡Œ build_osm_index.py --status æŸ¥çœ‹å„ç¸£å¸‚ä¸‹è¼‰é€²åº¦")
    else:
        print(f"   ç‹€æ…‹:              âŒ æœªå»ºç«‹ï¼ˆä½¿ç”¨è·¯æ®µç´šç²¾åº¦ï¼‰")
        print(f"   å•Ÿç”¨æ–¹æ³•ï¼š")
        print(f"     python3 build_osm_index.py  # ä¸‹è¼‰å…¨å°ç´„ 900 è¬ç­†é–€ç‰Œè³‡æ–™")
        print(f"     é è¨ˆè€—æ™‚ï¼š15-25 åˆ†é˜ï¼Œç©ºé–“éœ€æ±‚ï¼šç´„ 1.5-2 GB")

    print(f"\nğŸ—ºï¸  å‰ 10 å¤§å€åŸŸ:")
    for district, count in list(stats['districts'].items())[:10]:
        print(f"   {district:12s}  {count:>10,} ä¸åŒåœ°å€")

    print("=" * 60)


def main():
    parser = argparse.ArgumentParser(
        description='æ‰¹æ¬¡åœ°ç†ç·¨ç¢¼ land_a.db æ‰€æœ‰åœ°å€',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¯„ä¾‹:
  %(prog)s --status                              # æŸ¥çœ‹é€²åº¦
  %(prog)s --strategy road                       # è·¯æ®µç´šæ‰¹æ¬¡è™•ç†ï¼ˆæœ€å¿«ï¼‰
  %(prog)s --strategy road --limit 1000          # å…ˆæ¸¬è©¦ 1000 ç­†
  %(prog)s --strategy road --district æ¾å±±å€      # è™•ç†ç‰¹å®šå€åŸŸ
  %(prog)s --write-back                          # å¯«å› land_a.db
  %(prog)s --upgrade                             # å°‡è·¯æ®µç´šå‡ç´šç‚ºé–€ç‰Œç´šï¼ˆéœ€ OSM ç´¢å¼•ï¼‰
  %(prog)s --export result.csv                   # åŒ¯å‡º CSV
  %(prog)s --import-cache ../../geocode_cache.json  # åŒ¯å…¥èˆŠå¿«å–
  %(prog)s --strategy road --nominatim-url http://localhost:8080/search  # æœ¬åœ° Nominatim
        """
    )

    parser.add_argument('--status', action='store_true', help='é¡¯ç¤ºç›®å‰é€²åº¦')
    parser.add_argument('--strategy', choices=['smart', 'road', 'exact'],
                        default='smart', help='geocoding ç­–ç•¥ (é è¨­: smart)')
    parser.add_argument('--district', '-d', help='åªè™•ç†æŒ‡å®šå€åŸŸ')
    parser.add_argument('--limit', '-n', type=int, help='é™åˆ¶è™•ç†ç­†æ•¸')
    parser.add_argument('--provider', choices=['nominatim', 'nlsc'],
                        default='nominatim', help='API provider')
    parser.add_argument('--nominatim-url', help='æœ¬åœ° Nominatim URL')
    parser.add_argument('--write-back', action='store_true',
                        help='å°‡çµæœå¯«å› land_a.db')
    parser.add_argument('--upgrade', action='store_true',
                        help='å°‡ road ç´šåº§æ¨™å‡ç´šç‚ºé–€ç‰Œç´šï¼ˆéœ€å…ˆå»ºç«‹ OSM ç´¢å¼•ï¼‰')
    parser.add_argument('--dry-run', action='store_true',
                        help='[--upgrade é…åˆ] è©¦è·‘æ¨¡å¼ï¼Œä¸å¯¦éš›å¯«å…¥')
    parser.add_argument('--export', metavar='CSV', help='åŒ¯å‡ºçµæœç‚º CSV')
    parser.add_argument('--import-cache', metavar='JSON', help='åŒ¯å…¥ JSON å¿«å–')
    parser.add_argument('--db', default=DEFAULT_DB, help='land_a.db è·¯å¾‘')
    parser.add_argument('--verbose', '-v', action='store_true')

    args = parser.parse_args()

    if args.verbose:
        logging.basicConfig(level=logging.DEBUG, format='%(name)s: %(message)s')
    else:
        logging.basicConfig(level=logging.WARNING)

    # å»ºç«‹ geocoder
    gc = TaiwanGeocoder(
        provider=args.provider,
        nominatim_url=args.nominatim_url,
    )
    if gc.osm_index.is_available():
        print(f"ğŸ  OSM é–€ç‰Œç´¢å¼•ï¼š{gc.osm_index.node_count:,} å€‹ç¯€é»ï¼ˆç²¾ç¢ºé–€ç‰Œæ¨¡å¼ï¼‰")
    else:
        print(f"âš ï¸  OSM ç´¢å¼•æœªè¼‰å…¥ï¼Œä½¿ç”¨è·¯æ®µç´šç²¾åº¦ã€‚å»ºè­°å…ˆåŸ·è¡Œ build_osm_index.py")

    processor = LandDBProcessor(db_path=args.db, geocoder=gc)

    # â”€â”€ åŒ¯å…¥å¿«å– â”€â”€
    if args.import_cache:
        count = gc.cache.import_json_cache(args.import_cache)
        print(f"âœ… åŒ¯å…¥ {count:,} ç­†å¿«å–")
        return

    # â”€â”€ é¡¯ç¤ºç‹€æ…‹ â”€â”€
    if args.status:
        print_status(processor)
        return

    # â”€â”€ åŒ¯å‡º CSV â”€â”€
    if args.export:
        processor.export_csv(args.export, limit=args.limit)
        return

    # â”€â”€ å¯«å› land_a.db â”€â”€
    if args.write_back:
        processor.write_back()
        return

    # â”€â”€ å‡ç´š road â†’ exact â”€â”€
    if args.upgrade:
        processor.upgrade_road_to_exact(dry_run=args.dry_run)
        return

    # â”€â”€ æ‰¹æ¬¡ geocode â”€â”€
    print("=" * 60)
    print("ğŸŒ æ‰¹æ¬¡åœ°ç†ç·¨ç¢¼")
    print("=" * 60)

    start_time = time.time()

    # å–å¾—åœ°å€åˆ—è¡¨
    print(f"\nğŸ“– è®€å– land_a.db...")
    addresses = processor.get_unique_addresses(
        district=args.district,
        limit=args.limit
    )
    print(f"   å–å¾— {len(addresses):,} ç­†ä¸åŒåœ°å€")

    if not addresses:
        print("âš ï¸  æ²’æœ‰æ‰¾åˆ°éœ€è¦è™•ç†çš„åœ°å€")
        return

    # åŸ·è¡Œæ‰¹æ¬¡ geocode
    results = gc.batch_geocode(
        addresses,
        strategy=args.strategy,
        progress=True
    )

    elapsed = time.time() - start_time

    # â”€â”€ çµæœå ±å‘Š â”€â”€
    print(f"\n{'='*60}")
    print(f"ğŸ‰ æ‰¹æ¬¡è™•ç†å®Œæˆï¼")
    print(f"   è€—æ™‚: {elapsed:.1f} ç§’")
    print(f"   æˆåŠŸ: {len(results):,} / {len(addresses):,}")
    print(f"   æˆåŠŸç‡: {len(results)/max(len(addresses),1)*100:.1f}%")

    # çµ±è¨ˆç²¾åº¦åˆ†å¸ƒ
    levels = defaultdict(int)
    for r in results.values():
        levels[r.get('level', 'unknown')] += 1
    print(f"   ç²¾åº¦åˆ†å¸ƒ: {dict(levels)}")

    print(f"\nğŸ’¾ å¿«å–çµ±è¨ˆ:")
    cache_stats = gc.stats()
    print(f"   å¿«å–ç¸½æ•¸: {cache_stats['total']:,}")

    print(f"\nğŸ’¡ ä¸‹ä¸€æ­¥:")
    print(f"   python3 batch_geocode.py --write-back   # å¯«å› land_a.db")
    print(f"   python3 batch_geocode.py --export out.csv  # åŒ¯å‡º CSV")
    print(f"{'='*60}")


if __name__ == '__main__':
    main()
